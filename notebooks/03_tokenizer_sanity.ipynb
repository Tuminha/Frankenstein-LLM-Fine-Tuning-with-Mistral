{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer Sanity Check\n",
        "\n",
        "## Tokenizer Choice\n",
        "\n",
        "We'll use:\n",
        "- **Mistral-7B tokenizer** for QLoRA training (main path)\n",
        "- **DistilGPT-2 tokenizer** for CPU fallback\n",
        "\n",
        "Each model has its own tokenizer. The tokenizer determines:\n",
        "- How text is split into tokens\n",
        "- Vocabulary size\n",
        "- Special tokens (BOS, EOS, padding, etc.)\n",
        "\n",
        "## Left Padding for Causal LM\n",
        "\n",
        "Causal language models (like GPT, Mistral) generate left-to-right. For training:\n",
        "- **Right padding:** Standard for most tasks\n",
        "- **Left padding:** Sometimes used for batch efficiency\n",
        "\n",
        "We'll use **right padding** (default) for simplicity.\n",
        "\n",
        "## Max Length Trade-offs\n",
        "\n",
        "- **Too short:** Truncates context, loses information\n",
        "- **Too long:** Wastes memory, slower training\n",
        "\n",
        "For Mistral-7B, 512 tokens is a good balance. Check your data distribution first!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: It was on a dreary night of November that I beheld the accomplishment of my toils.\n",
            "Decoded: It was on a dreary night of November that I beheld the accomplishment of my toils.\n",
            "Round-trip match: True\n",
            "Token count: 22\n",
            "Character count: 82\n",
            "Average token length: 3.91\n",
            "Original: You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking. I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. Do you understand this feeling? This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes. Inspirited by this wind of promise, my daydreams become more fervent and vivid. I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight. There, Margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe. Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. What may not be expected in a country of eternal light? I may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever. I shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man. These are my enticements, and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday mates, on an expedition of discovery up his native river.\n",
            "Decoded: You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking. I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. Do you understand this feeling? This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes. Inspirited by this wind of promise, my daydreams become more fervent and vivid. I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight. There, Margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe. Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. What may not be expected in a country of eternal light? I may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever. I shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man. These are my enticements, and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday mates, on an expedition of discovery up his native river.\n",
            "Round-trip match: True\n",
            "Token count: 478\n",
            "Character count: 2097\n",
            "Average token length: 4.40\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load tokenizer (Mistral or DistilGPT2) and encode/decode a few samples.\n",
        "# Hints:\n",
        "#   - Use AutoTokenizer.from_pretrained()\n",
        "#   - Encode a sample text, then decode it back\n",
        "#   - Compare original vs decoded (should match for most cases)\n",
        "#   - Calculate average token length for a few samples\n",
        "# Acceptance:\n",
        "#   - round-trip decode matches expectations; reports avg token length\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "def tokenizer_roundtrip(base_model: str, seq_length: int, text: str):\n",
        "    \"\"\"\n",
        "    Test tokenizer encoding/decoding and report statistics.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name (e.g., \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "        seq_length: Maximum sequence length\n",
        "        text: Sample text to encode/decode\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    encoded = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=seq_length)\n",
        "    decoded = tokenizer.decode(encoded, skip_special_tokens=True)\n",
        "    # Fix: encoded is a list of token IDs (integers), not tokens themselves\n",
        "    # To get actual token strings, use convert_ids_to_tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded)\n",
        "    token_lengths = [len(token) for token in tokens]\n",
        "    print(f\"Original: {text}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(f\"Round-trip match: {decoded.strip() == text.strip()}\")\n",
        "    print(f\"Token count: {len(encoded)}\")\n",
        "    print(f\"Character count: {len(decoded)}\")\n",
        "    print(f\"Average token length: {np.mean(token_lengths):.2f}\")\n",
        "\n",
        "\n",
        "# Test Mistral tokenizer\n",
        "sample_text = \"It was on a dreary night of November that I beheld the accomplishment of my toils.\"\n",
        "tokenizer_roundtrip(\"mistralai/Mistral-7B-Instruct-v0.2\", seq_length=512, text=sample_text)\n",
        "# More challenging text with longer tokens\n",
        "sample_text_2 = \"\"\"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking. I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. Do you understand this feeling? This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes. Inspirited by this wind of promise, my daydreams become more fervent and vivid. I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight. There, Margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe. Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. What may not be expected in a country of eternal light? I may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever. I shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man. These are my enticements, and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday mates, on an expedition of discovery up his native river.\"\"\"\n",
        "tokenizer_roundtrip(\"mistralai/Mistral-7B-Instruct-v0.2\", seq_length=512, text=sample_text_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization Function for Dataset\n",
        "\n",
        "We need a function that tokenizes the entire dataset. This will be used in training.\n",
        "\n",
        "### Why do we need this function?\n",
        "\n",
        "When training a language model, we can't feed raw text directly. The model expects:\n",
        "- **Token IDs** (numbers representing words/subwords)\n",
        "- **Attention masks** (telling the model which tokens are real vs padding)\n",
        "\n",
        "### How `dataset.map()` works:\n",
        "\n",
        "The Hugging Face `datasets` library has a `.map()` method that applies a function to every example in the dataset. \n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Your dataset has examples like: {\"text\": \"It was a dark night...\"}\n",
        "# After tokenization, you want: {\"input_ids\": [1234, 567, 890, ...], \"attention_mask\": [1, 1, 1, ...]}\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function)\n",
        "```\n",
        "\n",
        "### What the function should do:\n",
        "\n",
        "1. **Take a batch** (or single example) from the dataset\n",
        "2. **Extract the text** from the 'text' column\n",
        "3. **Tokenize it** → convert text to token IDs\n",
        "4. **Handle truncation** → if text > 512 tokens, cut it off\n",
        "5. **Create attention mask** → mark which tokens are real (1) vs padding (0)\n",
        "6. **Return** a dict with 'input_ids' and 'attention_mask'\n",
        "\n",
        "### Key parameters:\n",
        "- `truncation=True`: Cut off text if too long\n",
        "- `max_length=seq_length`: Maximum tokens (512)\n",
        "- `padding=False`: We'll pad later during batching (more efficient)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual Example\n",
        "\n",
        "Here's what happens step by step:\n",
        "\n",
        "```python\n",
        "# Step 1: Your dataset has raw text\n",
        "example = {\"text\": \"It was a dark night\"}\n",
        "\n",
        "# Step 2: Tokenizer converts text to numbers\n",
        "tokenizer(\"It was a dark night\", truncation=True, max_length=512)\n",
        "# Returns:\n",
        "# {\n",
        "#   'input_ids': [1234, 567, 890, 123, 456],  # Token IDs\n",
        "#   'attention_mask': [1, 1, 1, 1, 1]          # All 1s (no padding yet)\n",
        "# }\n",
        "\n",
        "# Step 3: dataset.map() applies this to every example\n",
        "# Result: Each example now has 'input_ids' and 'attention_mask'\n",
        "```\n",
        "\n",
        "**Why `attention_mask`?**\n",
        "- `1` = real token (the model should pay attention)\n",
        "- `0` = padding token (ignore this, it's just filler)\n",
        "- Later, when batching, shorter sequences get padded with 0s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test result keys: ['input_ids', 'attention_mask']\n",
            "Input IDs shape: 11\n",
            "Attention mask shape: 11\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Prepare map() function to tokenize Dataset with truncation and optional packing.\n",
        "# Hints:\n",
        "#   - Create a function that takes a batch dict (or single example)\n",
        "#   - Extract 'text' from the batch\n",
        "#   - Use tokenizer() with truncation=True, max_length=seq_length, padding=False\n",
        "#   - Return dict with 'input_ids' and 'attention_mask'\n",
        "#   - This function will be used with dataset.map()\n",
        "# Acceptance:\n",
        "#   - returns tokenized dataset with 'input_ids' and 'attention_mask'\n",
        "\n",
        "# Example of what the function receives:\n",
        "# Input: {\"text\": \"It was a dark night...\"}\n",
        "# Output: {\"text\": \"It was a dark night...\", \"input_ids\": [1234, 567, 890, ...], \"attention_mask\": [1, 1, 1, ...]}\n",
        "\n",
        "def build_tokenize_fn(tokenizer, seq_length: int):\n",
        "    \"\"\"\n",
        "    Build a tokenization function for dataset.map().\n",
        "    \n",
        "    This function will be called by dataset.map() for each example (or batch).\n",
        "    It receives a dict with 'text' and returns the same dict + 'input_ids' + 'attention_mask'.\n",
        "    \n",
        "    Args:\n",
        "        tokenizer: Pre-trained tokenizer\n",
        "        seq_length: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        callable: Function that tokenizes a single example or batch\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        \"\"\"\n",
        "        This inner function is what dataset.map() will call.\n",
        "        \n",
        "        Args:\n",
        "            examples: Dict with 'text' key (can be single string or list of strings)\n",
        "            \n",
        "        Returns:\n",
        "            Dict with 'input_ids' and 'attention_mask' added\n",
        "        \"\"\"\n",
        "        # TODO: \n",
        "        # 1. Use tokenizer() on examples['text']\n",
        "        #    - Set truncation=True, max_length=seq_length, padding=False\n",
        "        #    - This returns a dict with 'input_ids' and 'attention_mask'\n",
        "        # 2. Return the tokenized result\n",
        "        #    - The tokenizer() already returns what we need!\n",
        "        result = tokenizer(\n",
        "            examples['text'], \n",
        "            truncation=True, \n",
        "            max_length=seq_length, \n",
        "            padding=False\n",
        "        )\n",
        "        return result\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # Return the tokenization function\n",
        "    return tokenize_function\n",
        "\n",
        "# Test it\n",
        "from datasets import load_dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "tokenize_fn = build_tokenize_fn(tokenizer, seq_length=512)\n",
        "\n",
        "# Test with a simple example\n",
        "test_example = {\"text\": \"It was on a dreary night of November that\"}\n",
        "result = tokenize_fn(test_example)\n",
        "print(\"Test result keys:\", list(result.keys()))\n",
        "print(\"Input IDs shape:\", len(result.get('input_ids', [])))\n",
        "print(\"Attention mask shape:\", len(result.get('attention_mask', [])))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

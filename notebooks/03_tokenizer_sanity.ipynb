{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer Sanity Check\n",
        "\n",
        "## Tokenizer Choice\n",
        "\n",
        "We'll use:\n",
        "- **Mistral-7B tokenizer** for QLoRA training (main path)\n",
        "- **DistilGPT-2 tokenizer** for CPU fallback\n",
        "\n",
        "Each model has its own tokenizer. The tokenizer determines:\n",
        "- How text is split into tokens\n",
        "- Vocabulary size\n",
        "- Special tokens (BOS, EOS, padding, etc.)\n",
        "\n",
        "## Left Padding for Causal LM\n",
        "\n",
        "Causal language models (like GPT, Mistral) generate left-to-right. For training:\n",
        "- **Right padding:** Standard for most tasks\n",
        "- **Left padding:** Sometimes used for batch efficiency\n",
        "\n",
        "We'll use **right padding** (default) for simplicity.\n",
        "\n",
        "## Max Length Trade-offs\n",
        "\n",
        "- **Too short:** Truncates context, loses information\n",
        "- **Too long:** Wastes memory, slower training\n",
        "\n",
        "For Mistral-7B, 512 tokens is a good balance. Check your data distribution first!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load tokenizer (Mistral or DistilGPT2) and encode/decode a few samples.\n",
        "# Hints:\n",
        "#   - Use AutoTokenizer.from_pretrained()\n",
        "#   - Encode a sample text, then decode it back\n",
        "#   - Compare original vs decoded (should match for most cases)\n",
        "#   - Calculate average token length for a few samples\n",
        "# Acceptance:\n",
        "#   - round-trip decode matches expectations; reports avg token length\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def tokenizer_roundtrip(base_model: str, seq_length: int):\n",
        "    \"\"\"\n",
        "    Test tokenizer encoding/decoding and report statistics.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name (e.g., \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "        seq_length: Maximum sequence length\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Test Mistral tokenizer\n",
        "tokenizer_roundtrip(\"mistralai/Mistral-7B-Instruct-v0.2\", seq_length=512)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization Function for Dataset\n",
        "\n",
        "We need a function that tokenizes the entire dataset. This will be used in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Prepare map() function to tokenize Dataset with truncation and optional packing.\n",
        "# Hints:\n",
        "#   - Create a function that takes a batch dict\n",
        "#   - Use tokenizer with truncation=True, max_length=seq_length\n",
        "#   - Return dict with 'input_ids' and 'attention_mask'\n",
        "#   - This function will be used with dataset.map()\n",
        "# Acceptance:\n",
        "#   - returns tokenized dataset with 'input_ids' and 'attention_mask'\n",
        "\n",
        "def build_tokenize_fn(tokenizer, seq_length: int):\n",
        "    \"\"\"\n",
        "    Build a tokenization function for dataset.map().\n",
        "    \n",
        "    Args:\n",
        "        tokenizer: Pre-trained tokenizer\n",
        "        seq_length: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        callable: Function that tokenizes a batch\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Test it\n",
        "from datasets import load_dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "tokenize_fn = build_tokenize_fn(tokenizer, seq_length=512)\n",
        "print(\"Tokenization function ready!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

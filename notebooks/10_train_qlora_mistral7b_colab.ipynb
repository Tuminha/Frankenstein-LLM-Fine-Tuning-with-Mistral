{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Training on Mistral-7B (GPU)\n",
        "\n",
        "**⚠️ REQUIRES GPU!** This notebook must be run in **Google Colab with GPU enabled** (Runtime → Change runtime type → GPU).\n",
        "\n",
        "**Why GPU is required:**\n",
        "- QLoRA still needs GPU for training (even with 4-bit quantization)\n",
        "- CPU training would take days/weeks and likely crash\n",
        "- GPU training takes ~30-60 minutes for 1 epoch\n",
        "\n",
        "**Recommended GPU:**\n",
        "- T4 (16GB) - works fine, free tier\n",
        "- A100 (80GB) - faster, paid tier (what you're using - excellent!)\n",
        "\n",
        "## What is QLoRA?\n",
        "\n",
        "**QLoRA** (Quantized Low-Rank Adaptation) combines:\n",
        "- **4-bit quantization:** Reduces model memory by ~75%\n",
        "- **LoRA (Low-Rank Adaptation):** Trains small adapter matrices instead of full weights\n",
        "\n",
        "Result: Train a 7B model on a T4 GPU (16GB) that normally requires 40GB+.\n",
        "\n",
        "## How 4-bit Quantization Works\n",
        "\n",
        "Instead of storing weights in FP32 (4 bytes), we use:\n",
        "- **4-bit integers:** 0.5 bytes per weight\n",
        "- **Quantization constants:** Small lookup tables to convert back\n",
        "\n",
        "This is lossy but preserves most model knowledge. Combined with LoRA, we get:\n",
        "- Fast training\n",
        "- Low memory usage\n",
        "- Good performance\n",
        "\n",
        "## Why T4 Fits\n",
        "\n",
        "Google Colab's T4 GPU has 16GB VRAM. With QLoRA:\n",
        "- Base model: ~4GB (4-bit)\n",
        "- LoRA adapters: ~100MB\n",
        "- Training overhead: ~8GB\n",
        "- **Total: ~12GB** ✅ Fits!\n",
        "\n",
        "## Hyperparameters in Plain English\n",
        "\n",
        "- **r (rank):** Size of adapter matrices. Higher = more capacity, more memory. r=8 is a good start.\n",
        "- **alpha:** Scaling factor. Usually alpha = 2*r. Controls adapter strength.\n",
        "- **dropout:** Regularization. 0.05 = 5% chance of dropping connections.\n",
        "- **lr:** Learning rate. 2e-4 is standard for LoRA.\n",
        "- **grad_accum:** Effective batch size = batch_size × grad_accum. Use 16 to simulate larger batches.\n",
        "\n",
        "## Avoiding OOM (Out of Memory)\n",
        "\n",
        "- Use gradient checkpointing\n",
        "- Keep batch_size=1, use grad_accum for effective batch\n",
        "- Use bfloat16 (more stable than float16)\n",
        "- Monitor GPU memory with `nvidia-smi`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Install GPU deps. Keep versions conservative. Verify CUDA is available.\n",
        "# Hints:\n",
        "#   - Install torch, transformers, peft, bitsandbytes, accelerate\n",
        "#   - Use !pip install in Colab\n",
        "#   - Check torch.cuda.is_available()\n",
        "# Acceptance:\n",
        "#   - torch.cuda.is_available() is True\n",
        "\n",
        "import torch\n",
        "\n",
        "def install_gpu_reqs():\n",
        "    \"\"\"\n",
        "    Install GPU dependencies and verify CUDA availability.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise ValueError(\"CUDA is not available. Please enable GPU in Colab.\")\n",
        "\n",
        "\n",
        "\n",
        "install_gpu_reqs()\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "\n",
        "Pull the dataset from the Hub (or load from local CSV if you didn't push it).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ⚠️ FOR COLAB USE: Replace the placeholder below with your actual HF token\n",
        "# In Colab, you can either:\n",
        "# 1. Replace \"YOUR_TOKEN_HERE\" with your actual token (temporary, for this session)\n",
        "# 2. Use: from huggingface_hub import login; login()  (recommended - stores token securely)\n",
        "# 3. Set as Colab secret: HF_TOKEN in Colab secrets (most secure)\n",
        "\n",
        "# Replace this placeholder with your actual token in Colab:\n",
        "HF_TOKEN = \"YOUR_TOKEN_HERE\"  # Replace with your actual token in Colab!\n",
        "\n",
        "# Alternative (recommended): Use login instead\n",
        "# from huggingface_hub import login\n",
        "# login()  # Enter token when prompted\n",
        "# Then use: from huggingface_hub import HfFolder; HF_TOKEN = HfFolder.get_token()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset from Hub: Tuminha/frankenstein-fanfic-snippets\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17149dea77bf48e7bbf8cd9608ad8785",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/456 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baa7bac2ce1046ecb3fc10c7a69eab83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized train: 456 samples\n",
            "Tokenized validation: 25 samples\n",
            "Train: 456, Val: 25\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load dataset from HF Hub or local CSV; tokenize with seq_length from config.\n",
        "# Hints:\n",
        "#   - Try load_dataset() first (Hub), fallback to CSV if needed\n",
        "#   - Tokenize using the function from notebook 03\n",
        "#   - Set padding token if missing\n",
        "# Acceptance:\n",
        "#   - tokenized train/validation Datasets ready for Trainer\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import os\n",
        "\n",
        "# Use the HF_TOKEN defined in cell 4 above\n",
        "hf_token = HF_TOKEN\n",
        "\n",
        "\n",
        "\n",
        "def load_and_tokenize(hub_id: str, base_model: str, seq_length: int):\n",
        "    \"\"\"\n",
        "    Load dataset from Hub or CSV and tokenize.\n",
        "    \n",
        "    Args:\n",
        "        hub_id: Hub dataset ID or path to CSV\n",
        "        base_model: Model name for tokenizer\n",
        "        seq_length: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (tokenized_train, tokenized_val) datasets\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading from Hub\n",
        "        dataset = load_dataset(hub_id, token=hf_token)\n",
        "        print(f\"Loaded dataset from Hub: {hub_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading from Hub: {e}\")\n",
        "        # Fallback to CSV\n",
        "        raise e\n",
        "\n",
        "    # Tokenize\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=seq_length)\n",
        "    \n",
        "    # Apply tokenization to both train and validation splits\n",
        "    tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    tokenized_val = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    \n",
        "    print(f\"Tokenized train: {len(tokenized_train)} samples\")\n",
        "    print(f\"Tokenized validation: {len(tokenized_val)} samples\")\n",
        "    \n",
        "    return tokenized_train, tokenized_val\n",
        "\n",
        "# Load and tokenize\n",
        "hub_id = \"Tuminha/frankenstein-fanfic-snippets\"  # or \"path/to/local.csv\"\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "ds_train, ds_val = load_and_tokenize(hub_id, base_model, seq_length=512)\n",
        "print(f\"Train: {len(ds_train)}, Val: {len(ds_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build 4-bit Model\n",
        "\n",
        "Load Mistral-7B in 4-bit mode using BitsAndBytes. This is the memory-saving step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Build 4-bit Mistral with BitsAndBytes and prepare for k-bit training.\n",
        "# Hints:\n",
        "#   - Use BitsAndBytesConfig with load_in_4bit=True\n",
        "#   - Load model with quantization_config\n",
        "#   - Enable gradient checkpointing to save memory\n",
        "#   - Set tokenizer padding side\n",
        "# Acceptance:\n",
        "#   - model loads on GPU; gradients checkpointed; memory < 16GB on T4\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def build_4bit_model(base_model: str):\n",
        "    \"\"\"\n",
        "    Load model in 4-bit quantization mode.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "\n",
        "model, tokenizer = build_4bit_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "print(\"4-bit model loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA and Train\n",
        "\n",
        "Set up LoRA adapters and training arguments. Then run one epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Create LoRA config and TrainingArguments; run one epoch.\n",
        "# Hints:\n",
        "#   - Use LoraConfig from peft with r/alpha/dropout from config\n",
        "#   - Set target_modules to attention layers\n",
        "#   - Use TrainingArguments with grad_accum, bf16, etc.\n",
        "#   - Use SFTTrainer from trl (or Trainer from transformers)\n",
        "# Acceptance:\n",
        "#   - training completes; loss decreases; adapter folder saved\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def train_qlora(model, tokenizer, ds_train, ds_val, cfg: dict, out_dir: str):\n",
        "    \"\"\"\n",
        "    Train LoRA adapters on 4-bit model.\n",
        "    \n",
        "    Args:\n",
        "        model: 4-bit quantized model\n",
        "        tokenizer: Tokenizer\n",
        "        ds_train: Training dataset\n",
        "        ds_val: Validation dataset\n",
        "        cfg: Config dict with qlora settings\n",
        "        out_dir: Output directory for adapters\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Train\n",
        "cfg = {\n",
        "    'qlora': {\n",
        "        'r': 8,\n",
        "        'alpha': 16,\n",
        "        'dropout': 0.05,\n",
        "        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
        "        'lr': 2.0e-4,\n",
        "        'grad_accum': 16,\n",
        "        'epochs': 1\n",
        "    }\n",
        "}\n",
        "train_qlora(model, tokenizer, ds_train, ds_val, cfg, out_dir=\"adapters/mistral-frankenstein\")\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push Adapters to Hub\n",
        "\n",
        "Save the adapters to the Hub so you can use them later (and share them).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Push the adapter to the Hub (private ok).\n",
        "# Hints:\n",
        "#   - Use adapter.push_to_hub() or model.push_to_hub()\n",
        "#   - Set private=True if desired\n",
        "#   - Include tokenizer if needed\n",
        "# Acceptance:\n",
        "#   - repo exists with adapter files; URL printed\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "def push_adapters(local_dir: str, repo_id: str):\n",
        "    \"\"\"\n",
        "    Push LoRA adapters to Hugging Face Hub.\n",
        "    \n",
        "    Args:\n",
        "        local_dir: Local directory with adapter files\n",
        "        repo_id: Hub repository ID\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "push_adapters(\"adapters/mistral-frankenstein\", \"YOURUSER/mistral-frankenstein-qlora\")\n",
        "print(\"Adapters pushed to Hub!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

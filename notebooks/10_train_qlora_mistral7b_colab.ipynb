{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Training on Mistral-7B (GPU)\n",
        "\n",
        "**⚠️ REQUIRES GPU!** This notebook must be run in **Google Colab with GPU enabled** (Runtime → Change runtime type → GPU).\n",
        "\n",
        "**Why GPU is required:**\n",
        "- QLoRA still needs GPU for training (even with 4-bit quantization)\n",
        "- CPU training would take days/weeks and likely crash\n",
        "- GPU training takes ~30-60 minutes for 1 epoch\n",
        "\n",
        "**Recommended GPU:**\n",
        "- T4 (16GB) - works fine, free tier\n",
        "- A100 (80GB) - faster, paid tier (what you're using - excellent!)\n",
        "\n",
        "## What is QLoRA?\n",
        "\n",
        "**QLoRA** (Quantized Low-Rank Adaptation) combines:\n",
        "- **4-bit quantization:** Reduces model memory by ~75%\n",
        "- **LoRA (Low-Rank Adaptation):** Trains small adapter matrices instead of full weights\n",
        "\n",
        "Result: Train a 7B model on a T4 GPU (16GB) that normally requires 40GB+.\n",
        "\n",
        "## How 4-bit Quantization Works\n",
        "\n",
        "Instead of storing weights in FP32 (4 bytes), we use:\n",
        "- **4-bit integers:** 0.5 bytes per weight\n",
        "- **Quantization constants:** Small lookup tables to convert back\n",
        "\n",
        "This is lossy but preserves most model knowledge. Combined with LoRA, we get:\n",
        "- Fast training\n",
        "- Low memory usage\n",
        "- Good performance\n",
        "\n",
        "## Why T4 Fits\n",
        "\n",
        "Google Colab's T4 GPU has 16GB VRAM. With QLoRA:\n",
        "- Base model: ~4GB (4-bit)\n",
        "- LoRA adapters: ~100MB\n",
        "- Training overhead: ~8GB\n",
        "- **Total: ~12GB** ✅ Fits!\n",
        "\n",
        "## Hyperparameters in Plain English\n",
        "\n",
        "- **r (rank):** Size of adapter matrices. Higher = more capacity, more memory. r=8 is a good start.\n",
        "- **alpha:** Scaling factor. Usually alpha = 2*r. Controls adapter strength.\n",
        "- **dropout:** Regularization. 0.05 = 5% chance of dropping connections.\n",
        "- **lr:** Learning rate. 2e-4 is standard for LoRA.\n",
        "- **grad_accum:** Effective batch size = batch_size × grad_accum. Use 16 to simulate larger batches.\n",
        "\n",
        "## Avoiding OOM (Out of Memory)\n",
        "\n",
        "- Use gradient checkpointing\n",
        "- Keep batch_size=1, use grad_accum for effective batch\n",
        "- Use bfloat16 (more stable than float16)\n",
        "- Monitor GPU memory with `nvidia-smi`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Install GPU deps. Keep versions conservative. Verify CUDA is available.\n",
        "# Hints:\n",
        "#   - Install torch, transformers, peft, bitsandbytes, accelerate\n",
        "#   - Use !pip install in Colab\n",
        "#   - Check torch.cuda.is_available()\n",
        "# Acceptance:\n",
        "#   - torch.cuda.is_available() is True\n",
        "\n",
        "import torch\n",
        "\n",
        "def install_gpu_reqs():\n",
        "    \"\"\"\n",
        "    Install GPU dependencies and verify CUDA availability.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise ValueError(\"CUDA is not available. Please enable GPU in Colab.\")\n",
        "\n",
        "\n",
        "\n",
        "install_gpu_reqs()\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "\n",
        "Pull the dataset from the Hub (or load from local CSV if you didn't push it).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ⚠️ FOR COLAB USE: Replace the placeholder below with your actual HF token\n",
        "# In Colab, you can either:\n",
        "# 1. Replace \"YOUR_TOKEN_HERE\" with your actual token (temporary, for this session)\n",
        "# 2. Use: from huggingface_hub import login; login()  (recommended - stores token securely)\n",
        "# 3. Set as Colab secret: HF_TOKEN in Colab secrets (most secure)\n",
        "\n",
        "# Replace this placeholder with your actual token in Colab:\n",
        "HF_TOKEN = \"YOUR_TOKEN_HERE\"  # Replace with your actual token in Colab!\n",
        "\n",
        "# Alternative (recommended): Use login instead\n",
        "# from huggingface_hub import login\n",
        "# login()  # Enter token when prompted\n",
        "# Then use: from huggingface_hub import HfFolder; HF_TOKEN = HfFolder.get_token()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset from Hub: Tuminha/frankenstein-fanfic-snippets\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17149dea77bf48e7bbf8cd9608ad8785",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/456 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baa7bac2ce1046ecb3fc10c7a69eab83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized train: 456 samples\n",
            "Tokenized validation: 25 samples\n",
            "Train: 456, Val: 25\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load dataset from HF Hub or local CSV; tokenize with seq_length from config.\n",
        "# Hints:\n",
        "#   - Try load_dataset() first (Hub), fallback to CSV if needed\n",
        "#   - Tokenize using the function from notebook 03\n",
        "#   - Set padding token if missing\n",
        "# Acceptance:\n",
        "#   - tokenized train/validation Datasets ready for Trainer\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import os\n",
        "\n",
        "# Use the HF_TOKEN defined in cell 4 above\n",
        "hf_token = HF_TOKEN\n",
        "\n",
        "\n",
        "\n",
        "def load_and_tokenize(hub_id: str, base_model: str, seq_length: int):\n",
        "    \"\"\"\n",
        "    Load dataset from Hub or CSV and tokenize.\n",
        "    \n",
        "    Args:\n",
        "        hub_id: Hub dataset ID or path to CSV\n",
        "        base_model: Model name for tokenizer\n",
        "        seq_length: Maximum sequence length\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (tokenized_train, tokenized_val) datasets\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading from Hub\n",
        "        dataset = load_dataset(hub_id, token=hf_token)\n",
        "        print(f\"Loaded dataset from Hub: {hub_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading from Hub: {e}\")\n",
        "        # Fallback to CSV\n",
        "        raise e\n",
        "\n",
        "    # Tokenize\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=seq_length)\n",
        "    \n",
        "    # Apply tokenization to both train and validation splits\n",
        "    tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    tokenized_val = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    \n",
        "    print(f\"Tokenized train: {len(tokenized_train)} samples\")\n",
        "    print(f\"Tokenized validation: {len(tokenized_val)} samples\")\n",
        "    \n",
        "    return tokenized_train, tokenized_val\n",
        "\n",
        "# Load and tokenize\n",
        "hub_id = \"Tuminha/frankenstein-fanfic-snippets\"  # or \"path/to/local.csv\"\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "ds_train, ds_val = load_and_tokenize(hub_id, base_model, seq_length=512)\n",
        "print(f\"Train: {len(ds_train)}, Val: {len(ds_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build 4-bit Model\n",
        "\n",
        "Load Mistral-7B in 4-bit mode using BitsAndBytes. This is the memory-saving step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ff3746b458d456aba31fac5ca776b31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2d3ab0e29174bb0be5be698e5e3cf3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac18271fc2345e8b08956b2ea6a4d04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f854be2a560f4fdba84133895130d713",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8d7412e288c438788899ada0bc31cba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ca6780b51614a8eafccf62642a5d590",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e1b63aa38454e37842ff94fe578b7df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c62f0cf185c74cef8f870682ab793286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4-bit model loaded!\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Build 4-bit Mistral with BitsAndBytes and prepare for k-bit training.\n",
        "# Hints:\n",
        "#   - Use BitsAndBytesConfig with load_in_4bit=True\n",
        "#   - Load model with quantization_config\n",
        "#   - Enable gradient checkpointing to save memory\n",
        "#   - Set tokenizer padding side\n",
        "# Acceptance:\n",
        "#   - model loads on GPU; gradients checkpointed; memory < 16GB on T4\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def build_4bit_model(base_model: str):\n",
        "    \"\"\"\n",
        "    Load model in 4-bit quantization mode.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    \n",
        "    model.gradient_checkpointing_enable()   \n",
        "    model.enable_input_require_grads()\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = build_4bit_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "print(\"4-bit model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this in a cell before Cell 8:\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"wandb\", \"-y\"])\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Disabled wandb (not needed for training)\n",
            "✅ wandb not installed - good!\n",
            "\n",
            "✅ Ready to continue! wandb is disabled/uninstalled.\n",
            "   You can proceed to the next cell - SFTTrainer should work now.\n"
          ]
        }
      ],
      "source": [
        "# === Fix wandb/trl compatibility issue ===\n",
        "# If you get error: \"module 'wandb.sdk' has no attribute 'lib'\"\n",
        "# SOLUTION: Disable wandb (we don't need it for training - it's just for logging)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Disable wandb to avoid compatibility issues\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "print(\"✅ Disabled wandb (not needed for training)\")\n",
        "\n",
        "# Optionally uninstall wandb if it's causing issues\n",
        "try:\n",
        "    import wandb\n",
        "    print(\"⚠️  wandb is installed. Uninstalling to avoid conflicts...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"wandb\", \"-y\", \"--quiet\"],\n",
        "                         stderr=subprocess.DEVNULL)\n",
        "    print(\"✅ Uninstalled wandb\")\n",
        "except ImportError:\n",
        "    print(\"✅ wandb not installed - good!\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"⚠️  Could not uninstall wandb (may not be installed) - that's fine\")\n",
        "\n",
        "print(\"\\n✅ Ready to continue! wandb is disabled/uninstalled.\")\n",
        "print(\"   You can proceed to the next cell - SFTTrainer should work now.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA and Train\n",
        "\n",
        "Set up LoRA adapters and training arguments. Then run one epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4060162095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     }\n\u001b[1;32m    129\u001b[0m }\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mtrain_qlora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adapters/mistral-frankenstein\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4060162095.py\u001b[0m in \u001b[0;36mtrain_qlora\u001b[0;34m(model, tokenizer, ds_train, ds_val, cfg, out_dir)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
          ]
        }
      ],
      "source": [
        "# === Fix wandb compatibility issue ===\n",
        "# wandb version conflict with trl. Fix by installing compatible version.\n",
        "# Run this cell first if you get: \"module 'wandb.sdk' has no attribute 'lib'\"\n",
        "try:\n",
        "    import wandb\n",
        "    # Check if wandb is causing issues\n",
        "    if hasattr(wandb, 'sdk') and not hasattr(wandb.sdk, 'lib'):\n",
        "        print(\"⚠️  wandb compatibility issue detected. Fixing...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wandb==0.15.12\", \"--quiet\"])\n",
        "        print(\"✅ Fixed wandb version. Please restart runtime: Runtime → Restart runtime\")\n",
        "        print(\"   Then run this cell again.\")\n",
        "except ImportError:\n",
        "    pass  # wandb not installed, that's fine\n",
        "\n",
        "# === TODO (you code this) ===\n",
        "# Create LoRA config and TrainingArguments; run one epoch.\n",
        "# Hints:\n",
        "#   - Use LoraConfig from peft with r/alpha/dropout from config\n",
        "#   - Set target_modules to attention layers\n",
        "#   - Use TrainingArguments with grad_accum, bf16, etc.\n",
        "#   - Use SFTTrainer from trl (or Trainer from transformers)\n",
        "# Acceptance:\n",
        "#   - training completes; loss decreases; adapter folder saved\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Try to import SFTTrainer, with fallback to regular Trainer if wandb issue persists\n",
        "try:\n",
        "    from trl import SFTTrainer\n",
        "    USE_SFT_TRAINER = True\n",
        "except RuntimeError as e:\n",
        "    if \"wandb\" in str(e).lower():\n",
        "        print(\"⚠️  wandb issue detected. Installing compatible version...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wandb==0.15.12\", \"--quiet\", \"--upgrade\"])\n",
        "        print(\"✅ Installed wandb==0.15.12. Please restart runtime: Runtime → Restart runtime\")\n",
        "        print(\"   Then run this cell again.\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "except ImportError:\n",
        "    print(\"⚠️  SFTTrainer not available. Using regular Trainer instead.\")\n",
        "    USE_SFT_TRAINER = False\n",
        "\n",
        "def train_qlora(model, tokenizer, ds_train, ds_val, cfg: dict, out_dir: str):\n",
        "    \"\"\"\n",
        "    Train LoRA adapters on 4-bit model.\n",
        "    \n",
        "    Args:\n",
        "        model: 4-bit quantized model\n",
        "        tokenizer: Tokenizer\n",
        "        ds_train: Training dataset\n",
        "        ds_val: Validation dataset\n",
        "        cfg: Config dict with qlora settings\n",
        "        out_dir: Output directory for adapters\n",
        "    \"\"\"\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    lora_config = LoraConfig(\n",
        "        r=cfg['qlora']['r'],\n",
        "        lora_alpha=cfg['qlora']['alpha'],\n",
        "        lora_dropout=cfg['qlora']['dropout'],\n",
        "        target_modules=cfg['qlora']['target_modules'],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, lora_config)\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=cfg['train']['batch_size'],\n",
        "        per_device_eval_batch_size=cfg['train']['batch_size'],\n",
        "        learning_rate=cfg['qlora']['lr'],\n",
        "        weight_decay=0.01,\n",
        "        num_train_epochs=cfg['qlora']['epochs'],\n",
        "        gradient_accumulation_steps=cfg['qlora']['grad_accum'],\n",
        "        gradient_checkpointing=True,\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        logging_dir=f\"{out_dir}/logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_safetensors=True,\n",
        "    )\n",
        "    \n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=ds_train,\n",
        "        eval_dataset=ds_val,\n",
        "        args=training_args,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=512,\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    trainer.save_model(out_dir)\n",
        "    \n",
        "    \n",
        "\n",
        "# Train\n",
        "cfg = {\n",
        "    'qlora': {\n",
        "        'r': 8,\n",
        "        'alpha': 16,\n",
        "        'dropout': 0.05,\n",
        "        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
        "        'lr': 2.0e-4,\n",
        "        'grad_accum': 16,\n",
        "        'epochs': 1\n",
        "    },\n",
        "    'train': {\n",
        "        'batch_size': 1\n",
        "    }\n",
        "}\n",
        "train_qlora(model, tokenizer, ds_train, ds_val, cfg, out_dir=\"adapters/mistral-frankenstein\")\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push Adapters to Hub\n",
        "\n",
        "Save the adapters to the Hub so you can use them later (and share them).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Push the adapter to the Hub (private ok).\n",
        "# Hints:\n",
        "#   - Use adapter.push_to_hub() or model.push_to_hub()\n",
        "#   - Set private=True if desired\n",
        "#   - Include tokenizer if needed\n",
        "# Acceptance:\n",
        "#   - repo exists with adapter files; URL printed\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "def push_adapters(local_dir: str, repo_id: str):\n",
        "    \"\"\"\n",
        "    Push LoRA adapters to Hugging Face Hub.\n",
        "    \n",
        "    Args:\n",
        "        local_dir: Local directory with adapter files\n",
        "        repo_id: Hub repository ID\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "push_adapters(\"adapters/mistral-frankenstein\", \"YOURUSER/mistral-frankenstein-qlora\")\n",
        "print(\"Adapters pushed to Hub!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Training on Mistral-7B (GPU)\n",
        "\n",
        "**⚠️ REQUIRES GPU!** This notebook must be run in **Google Colab with GPU enabled** (Runtime → Change runtime type → GPU).\n",
        "\n",
        "**Why GPU is required:**\n",
        "- QLoRA still needs GPU for training (even with 4-bit quantization)\n",
        "- CPU training would take days/weeks and likely crash\n",
        "- GPU training takes ~30-60 minutes for 1 epoch\n",
        "\n",
        "**Recommended GPU:**\n",
        "- T4 (16GB) - works fine, free tier\n",
        "- A100 (80GB) - faster, paid tier (what you're using - excellent!)\n",
        "\n",
        "## What is QLoRA?\n",
        "\n",
        "**QLoRA** (Quantized Low-Rank Adaptation) combines:\n",
        "- **4-bit quantization:** Reduces model memory by ~75%\n",
        "- **LoRA (Low-Rank Adaptation):** Trains small adapter matrices instead of full weights\n",
        "\n",
        "Result: Train a 7B model on a T4 GPU (16GB) that normally requires 40GB+.\n",
        "\n",
        "## How 4-bit Quantization Works\n",
        "\n",
        "Instead of storing weights in FP32 (4 bytes), we use:\n",
        "- **4-bit integers:** 0.5 bytes per weight\n",
        "- **Quantization constants:** Small lookup tables to convert back\n",
        "\n",
        "This is lossy but preserves most model knowledge. Combined with LoRA, we get:\n",
        "- Fast training\n",
        "- Low memory usage\n",
        "- Good performance\n",
        "\n",
        "## Why T4 Fits\n",
        "\n",
        "Google Colab's T4 GPU has 16GB VRAM. With QLoRA:\n",
        "- Base model: ~4GB (4-bit)\n",
        "- LoRA adapters: ~100MB\n",
        "- Training overhead: ~8GB\n",
        "- **Total: ~12GB** ✅ Fits!\n",
        "\n",
        "## Hyperparameters in Plain English\n",
        "\n",
        "- **r (rank):** Size of adapter matrices. Higher = more capacity, more memory. r=8 is a good start.\n",
        "- **alpha:** Scaling factor. Usually alpha = 2*r. Controls adapter strength.\n",
        "- **dropout:** Regularization. 0.05 = 5% chance of dropping connections.\n",
        "- **lr:** Learning rate. 2e-4 is standard for LoRA.\n",
        "- **grad_accum:** Effective batch size = batch_size × grad_accum. Use 16 to simulate larger batches.\n",
        "\n",
        "## Avoiding OOM (Out of Memory)\n",
        "\n",
        "- Use gradient checkpointing\n",
        "- Keep batch_size=1, use grad_accum for effective batch\n",
        "- Use bfloat16 (more stable than float16)\n",
        "- Monitor GPU memory with `nvidia-smi`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Install GPU deps. Keep versions conservative. Verify CUDA is available.\n",
        "# Hints:\n",
        "#   - Install torch, transformers, peft, bitsandbytes, accelerate\n",
        "#   - Use !pip install in Colab\n",
        "#   - Check torch.cuda.is_available()\n",
        "# Acceptance:\n",
        "#   - torch.cuda.is_available() is True\n",
        "\n",
        "import torch\n",
        "\n",
        "def install_gpu_reqs():\n",
        "    \"\"\"\n",
        "    Install GPU dependencies and verify CUDA availability.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise ValueError(\"CUDA is not available. Please enable GPU in Colab.\")\n",
        "\n",
        "\n",
        "\n",
        "install_gpu_reqs()\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "\n",
        "Pull the dataset from the Hub (or load from local CSV if you didn't push it).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ⚠️ FOR COLAB USE: Replace the placeholder below with your actual HF token\n",
        "# In Colab, you can either:\n",
        "# 1. Replace \"YOUR_TOKEN_HERE\" with your actual token (temporary, for this session)\n",
        "# 2. Use: from huggingface_hub import login; login()  (recommended - stores token securely)\n",
        "# 3. Set as Colab secret: HF_TOKEN in Colab secrets (most secure)\n",
        "\n",
        "# Replace this placeholder with your actual token in Colab:\n",
        "HF_TOKEN = \"token\"  # Replace with your actual token in Colab!\n",
        "\n",
        "# Alternative (recommended): Use login instead\n",
        "# from huggingface_hub import login\n",
        "# login()  # Enter token when prompted\n",
        "# Then use: from huggingface_hub import HfFolder; HF_TOKEN = HfFolder.get_token()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded dataset from Hub: Tuminha/frankenstein-fanfic-snippets\n",
            "✅ Train dataset: 456 samples (raw text)\n",
            "✅ Validation dataset: 25 samples (raw text)\n",
            "   Note: SFTTrainer will tokenize automatically during training\n",
            "Train: 456, Val: 25\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load dataset from HF Hub or local CSV; tokenize with seq_length from config.\n",
        "# Hints:\n",
        "#   - Try load_dataset() first (Hub), fallback to CSV if needed\n",
        "#   - Tokenize using the function from notebook 03\n",
        "#   - Set padding token if missing\n",
        "# Acceptance:\n",
        "#   - tokenized train/validation Datasets ready for Trainer\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import os\n",
        "\n",
        "# Use the HF_TOKEN defined in cell 4 above\n",
        "hf_token = HF_TOKEN\n",
        "\n",
        "\n",
        "\n",
        "def load_and_tokenize(hub_id: str, base_model: str, seq_length: int):\n",
        "    \"\"\"\n",
        "    Load dataset from Hub or CSV.\n",
        "    \n",
        "    NOTE: SFTTrainer expects raw text (not tokenized) and does its own tokenization.\n",
        "    So we just load the dataset without tokenizing it.\n",
        "    \n",
        "    Args:\n",
        "        hub_id: Hub dataset ID or path to CSV\n",
        "        base_model: Model name for tokenizer (not used here, but kept for compatibility)\n",
        "        seq_length: Maximum sequence length (not used here, but kept for compatibility)\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (train_dataset, val_dataset) with raw text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading from Hub\n",
        "        dataset = load_dataset(hub_id, token=hf_token)\n",
        "        print(f\"✅ Loaded dataset from Hub: {hub_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading from Hub: {e}\")\n",
        "        # Fallback to CSV\n",
        "        raise e\n",
        "\n",
        "    # SFTTrainer expects raw text, so we don't tokenize here\n",
        "    # It will handle tokenization internally\n",
        "    train_dataset = dataset[\"train\"]\n",
        "    val_dataset = dataset[\"validation\"]\n",
        "    \n",
        "    print(f\"✅ Train dataset: {len(train_dataset)} samples (raw text)\")\n",
        "    print(f\"✅ Validation dataset: {len(val_dataset)} samples (raw text)\")\n",
        "    print(\"   Note: SFTTrainer will tokenize automatically during training\")\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Load and tokenize\n",
        "hub_id = \"Tuminha/frankenstein-fanfic-snippets\"  # or \"path/to/local.csv\"\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "ds_train, ds_val = load_and_tokenize(hub_id, base_model, seq_length=512)\n",
        "print(f\"Train: {len(ds_train)}, Val: {len(ds_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build 4-bit Model\n",
        "\n",
        "Load Mistral-7B in 4-bit mode using BitsAndBytes. This is the memory-saving step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ef83931fe7647f481ef98a2fc38aaa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4-bit model loaded!\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Build 4-bit Mistral with BitsAndBytes and prepare for k-bit training.\n",
        "# Hints:\n",
        "#   - Use BitsAndBytesConfig with load_in_4bit=True\n",
        "#   - Load model with quantization_config\n",
        "#   - Enable gradient checkpointing to save memory\n",
        "#   - Set tokenizer padding side\n",
        "# Acceptance:\n",
        "#   - model loads on GPU; gradients checkpointed; memory < 16GB on T4\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def build_4bit_model(base_model: str):\n",
        "    \"\"\"\n",
        "    Load model in 4-bit quantization mode.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    \n",
        "    model.gradient_checkpointing_enable()   \n",
        "    model.enable_input_require_grads()\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = build_4bit_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "print(\"4-bit model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this in a cell before Cell 8:\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"wandb\", \"-y\"])\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Disabled wandb (not needed for training)\n",
            "✅ wandb not installed - good!\n",
            "\n",
            "✅ Ready to continue! wandb is disabled/uninstalled.\n",
            "   You can proceed to the next cell - SFTTrainer should work now.\n"
          ]
        }
      ],
      "source": [
        "# === Fix wandb/trl compatibility issue ===\n",
        "# If you get error: \"module 'wandb.sdk' has no attribute 'lib'\"\n",
        "# SOLUTION: Disable wandb (we don't need it for training - it's just for logging)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Disable wandb to avoid compatibility issues\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "print(\"✅ Disabled wandb (not needed for training)\")\n",
        "\n",
        "# Optionally uninstall wandb if it's causing issues\n",
        "try:\n",
        "    import wandb\n",
        "    print(\"⚠️  wandb is installed. Uninstalling to avoid conflicts...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"wandb\", \"-y\", \"--quiet\"],\n",
        "                         stderr=subprocess.DEVNULL)\n",
        "    print(\"✅ Uninstalled wandb\")\n",
        "except ImportError:\n",
        "    print(\"✅ wandb not installed - good!\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"⚠️  Could not uninstall wandb (may not be installed) - that's fine\")\n",
        "\n",
        "print(\"\\n✅ Ready to continue! wandb is disabled/uninstalled.\")\n",
        "print(\"   You can proceed to the next cell - SFTTrainer should work now.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA and Train\n",
        "\n",
        "Set up LoRA adapters and training arguments. Then run one epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to create SFTTrainer...\n",
            "✅ SFTTrainer created successfully with minimal parameters\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 03:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Training logs saved to ../adapters/mistral-frankenstein/logs/trainer_state.json\n",
            "Validation losses: [{'loss': 2.2292, 'grad_norm': 1.2944529056549072, 'learning_rate': 0.00016234898018587337, 'entropy': 2.113223755918443, 'num_tokens': 37481.0, 'mean_token_accuracy': 0.5097053039819002, 'epoch': 0.3508771929824561, 'step': 10}, {'loss': 1.9667, 'grad_norm': 1.1977261304855347, 'learning_rate': 5.6611626088244194e-05, 'entropy': 1.9625635132193566, 'num_tokens': 74539.0, 'mean_token_accuracy': 0.5463880322873592, 'epoch': 0.7017543859649122, 'step': 20}, {'train_runtime': 222.8623, 'train_samples_per_second': 2.046, 'train_steps_per_second': 0.13, 'total_flos': 4565576052326400.0, 'train_loss': 2.0713837722252153, 'entropy': 2.0800778458223625, 'num_tokens': 106910.0, 'mean_token_accuracy': 0.5315580129185143, 'epoch': 1.0, 'step': 29}]\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# === Fix wandb compatibility issue ===\n",
        "# wandb version conflict with trl. Fix by installing compatible version.\n",
        "# Run this cell first if you get: \"module 'wandb.sdk' has no attribute 'lib'\"\n",
        "try:\n",
        "    import wandb\n",
        "    # Check if wandb is causing issues\n",
        "    if hasattr(wandb, 'sdk') and not hasattr(wandb.sdk, 'lib'):\n",
        "        print(\"⚠️  wandb compatibility issue detected. Fixing...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wandb==0.15.12\", \"--quiet\"])\n",
        "        print(\"✅ Fixed wandb version. Please restart runtime: Runtime → Restart runtime\")\n",
        "        print(\"   Then run this cell again.\")\n",
        "except ImportError:\n",
        "    pass  # wandb not installed, that's fine\n",
        "\n",
        "# === TODO (you code this) ===\n",
        "# Create LoRA config and TrainingArguments; run one epoch.\n",
        "# Hints:\n",
        "#   - Use LoraConfig from peft with r/alpha/dropout from config\n",
        "#   - Set target_modules to attention layers\n",
        "#   - Use TrainingArguments with grad_accum, bf16, etc.\n",
        "#   - Use SFTTrainer from trl (or Trainer from transformers)\n",
        "# Acceptance:\n",
        "#   - training completes; loss decreases; adapter folder saved\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Try to import SFTTrainer, with fallback to regular Trainer if wandb issue persists\n",
        "try:\n",
        "    from trl import SFTTrainer\n",
        "    USE_SFT_TRAINER = True\n",
        "except RuntimeError as e:\n",
        "    if \"wandb\" in str(e).lower():\n",
        "        print(\"⚠️  wandb issue detected. Installing compatible version...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wandb==0.15.12\", \"--quiet\", \"--upgrade\"])\n",
        "        print(\"✅ Installed wandb==0.15.12. Please restart runtime: Runtime → Restart runtime\")\n",
        "        print(\"   Then run this cell again.\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "except ImportError:\n",
        "    print(\"⚠️  SFTTrainer not available. Using regular Trainer instead.\")\n",
        "    USE_SFT_TRAINER = False\n",
        "\n",
        "\n",
        "def train_qlora(model, tokenizer, ds_train, ds_val, cfg: dict, out_dir: str):\n",
        "    \"\"\"\n",
        "    Train LoRA adapters on 4-bit model.\n",
        "\n",
        "    Args:\n",
        "        model: 4-bit quantized model\n",
        "        tokenizer: Tokenizer\n",
        "        ds_train: Training dataset\n",
        "        ds_val: Validation dataset\n",
        "        cfg: Config dict with qlora settings\n",
        "        out_dir: Output directory for adapters\n",
        "    \"\"\"\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=cfg['qlora']['r'],\n",
        "        lora_alpha=cfg['qlora']['alpha'],\n",
        "        lora_dropout=cfg['qlora']['dropout'],\n",
        "        target_modules=cfg['qlora']['target_modules'],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=cfg['train']['batch_size'],\n",
        "        per_device_eval_batch_size=cfg['train']['batch_size'],\n",
        "        learning_rate=cfg['qlora']['lr'],\n",
        "        weight_decay=0.01,\n",
        "        num_train_epochs=cfg['qlora']['epochs'],\n",
        "        gradient_accumulation_steps=cfg['qlora']['grad_accum'],\n",
        "        gradient_checkpointing=True,\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.03,\n",
        "        logging_dir=f\"{out_dir}/logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_safetensors=True,\n",
        "    )\n",
        "\n",
        "    # SFTTrainer has different APIs in different trl versions\n",
        "    # Try minimal parameters first, fallback to regular Trainer if needed\n",
        "    print(\"Attempting to create SFTTrainer...\")\n",
        "\n",
        "    try:\n",
        "        # Try with minimal parameters (most compatible)\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,\n",
        "            train_dataset=ds_train,\n",
        "            eval_dataset=ds_val,\n",
        "            args=training_args,\n",
        "        )\n",
        "        print(\"✅ SFTTrainer created successfully with minimal parameters\")\n",
        "    except TypeError as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"⚠️  SFTTrainer error: {error_msg}\")\n",
        "\n",
        "        if \"unexpected keyword\" in error_msg:\n",
        "            print(\"   SFTTrainer API doesn't match this trl version.\")\n",
        "            print(\"   Falling back to regular Trainer (will need to tokenize dataset first)...\")\n",
        "\n",
        "            # Fallback: Use regular Trainer with pre-tokenized dataset\n",
        "            from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "            # Tokenize the datasets\n",
        "            def tokenize_function(examples):\n",
        "                return tokenizer(\n",
        "                    examples[\"text\"],\n",
        "                    truncation=True,\n",
        "                    max_length=512,\n",
        "                    padding=False,  # We'll use data collator for padding\n",
        "                )\n",
        "\n",
        "            print(\"   Tokenizing datasets...\")\n",
        "            tokenized_train = ds_train.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "            tokenized_val = ds_val.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "            # Create data collator for dynamic padding\n",
        "            data_collator = DataCollatorForLanguageModeling(\n",
        "                tokenizer=tokenizer,\n",
        "                mlm=False,  # Causal LM, not masked LM\n",
        "            )\n",
        "\n",
        "            # Use regular Trainer\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                train_dataset=tokenized_train,\n",
        "                eval_dataset=tokenized_val,\n",
        "                args=training_args,\n",
        "                data_collator=data_collator,\n",
        "            )\n",
        "            print(\"✅ Using regular Trainer with pre-tokenized dataset\")\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    trainer.save_model(out_dir)\n",
        "\n",
        "    # Save training logs\n",
        "    import json\n",
        "    log_dir = f\"{out_dir}/logs\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    log_path = f\"{log_dir}/trainer_state.json\"\n",
        "\n",
        "    # Save the trainer state to the log file\n",
        "    if hasattr(trainer, 'state') and trainer.state is not None:\n",
        "        with open(log_path, 'w') as f:\n",
        "            json.dump(trainer.state.log_history, f, indent=2)\n",
        "        print(f\"✅ Training logs saved to {log_path}\")\n",
        "        print(\"Validation losses:\", trainer.state.log_history)\n",
        "    else:\n",
        "        print(\"⚠️ Trainer state not available\")\n",
        "\n",
        "\n",
        "# Train\n",
        "cfg = {\n",
        "    'qlora': {\n",
        "        'r': 8,\n",
        "        'alpha': 16,\n",
        "        'dropout': 0.05,\n",
        "        'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
        "        'lr': 2.0e-4,\n",
        "        'grad_accum': 16,\n",
        "        'epochs': 1\n",
        "    },\n",
        "    'train': {\n",
        "        'batch_size': 1\n",
        "    }\n",
        "}\n",
        "train_qlora(model, tokenizer, ds_train, ds_val, cfg, out_dir=\"../adapters/mistral-frankenstein\")\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push Adapters to Hub\n",
        "\n",
        "Save the adapters to the Hub so you can use them later (and share them).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bbc4e389936423baed4d832b16b199f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "256a82e33b2f41518d6efa9501b9219f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8775cdc42ef4410ca560d663da6ad6dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...adapter_model.safetensors:  16%|#6        | 4.46MB / 27.3MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4018e1a5d9fd4c0b9cdc6c8dc2b8f810",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c08d414b9374de49124ac6ac1db2a98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeb47773f6a04aa2a8ce204718556121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "401557068755488195c5ca15de42b454",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...pgsbd08ta/tokenizer.model: 100%|##########|  493kB /  493kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Adapters pushed to Hub: https://huggingface.co/Tuminha/mistral-frankenstein-qlora\n",
            "Adapters pushed to Hub!\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Push the adapter to the Hub (private ok).\n",
        "# Hints:\n",
        "#   - Use adapter.push_to_hub() or model.push_to_hub()\n",
        "#   - Set private=True if desired\n",
        "#   - Include tokenizer if needed\n",
        "# Acceptance:\n",
        "#   - repo exists with adapter files; URL printed\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "def push_adapters(base_model, local_dir: str, repo_id: str, private: bool = False):\n",
        "    \"\"\"\n",
        "    Push LoRA adapters to Hugging Face Hub.\n",
        "    \n",
        "    Args:\n",
        "        base_model: The base model to load adapters onto\n",
        "        local_dir: Local directory with adapter files\n",
        "        repo_id: Hub repository ID\n",
        "        private: Whether to make the repo private\n",
        "    \"\"\"\n",
        "    # Load the adapter onto the base model\n",
        "    peft_model = PeftModel.from_pretrained(base_model, local_dir)\n",
        "    \n",
        "    # Push to hub\n",
        "    peft_model.push_to_hub(repo_id, private=private)\n",
        "    \n",
        "    # Also push tokenizer for convenience\n",
        "    tokenizer.push_to_hub(repo_id, private=private)\n",
        "    \n",
        "    print(f\"✅ Adapters pushed to Hub: https://huggingface.co/{repo_id}\")\n",
        "    \n",
        "\n",
        "push_adapters(model, \"adapters/mistral-frankenstein\", \"Tuminha/mistral-frankenstein-qlora\", private=False)\n",
        "print(\"Adapters pushed to Hub!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

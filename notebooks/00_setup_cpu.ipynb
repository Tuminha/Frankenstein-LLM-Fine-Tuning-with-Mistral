{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup: Hybrid CPU→GPU Workflow\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project finetunes a language model (Mistral-7B via QLoRA) to generate Frankenstein-style fanfiction. The workflow is **hybrid**:\n",
        "\n",
        "- **CPU phase (local/IDE):** Data exploration, cleaning, tokenizer checks, and evaluation harness\n",
        "- **GPU phase (Colab/Kaggle/RunPod):** QLoRA training on Mistral-7B\n",
        "\n",
        "## Why Hybrid?\n",
        "\n",
        "Most ML work doesn't need a GPU:\n",
        "- Loading and exploring data\n",
        "- Tokenization and dataset preparation\n",
        "- Evaluation metrics (perplexity, sample generation)\n",
        "\n",
        "Only the actual training benefits from GPU acceleration. By splitting the workflow, you can:\n",
        "- Work comfortably in your local IDE\n",
        "- Use free GPU resources (Colab) only when needed\n",
        "- Keep costs low\n",
        "\n",
        "## The Hub as a \"Bus\"\n",
        "\n",
        "We use the **Hugging Face Hub** to shuttle data between environments:\n",
        "1. **CPU → Hub:** Push your cleaned dataset\n",
        "2. **Hub → GPU:** Pull dataset in Colab\n",
        "3. **GPU → Hub:** Push trained adapters\n",
        "4. **Hub → CPU:** Pull adapters for local evaluation (optional)\n",
        "\n",
        "This makes the workflow portable and reproducible.\n",
        "\n",
        "## Environment Setup\n",
        "\n",
        "First, let's set up the local CPU environment and load configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load YAML config from configs/train.yaml into a dict.\n",
        "# Hints:\n",
        "#   - Use yaml.safe_load() or ruamel.yaml\n",
        "#   - Handle missing file gracefully\n",
        "#   - Return a nested dict matching the YAML structure\n",
        "# Acceptance:\n",
        "#   - dict has keys ['dataset_csv','hub_dataset_id','seq_length',...]\n",
        "#   - Can access nested values like cfg['qlora']['r']\n",
        "\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "def load_config(path=\"configs/train.yaml\"):\n",
        "    \"\"\"\n",
        "    Load YAML configuration file into a dictionary.\n",
        "    \n",
        "    Args:\n",
        "        path: Path to YAML config file\n",
        "        \n",
        "    Returns:\n",
        "        dict: Configuration dictionary\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Test it\n",
        "cfg = load_config()\n",
        "print(\"Config loaded:\", list(cfg.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Directory Structure\n",
        "\n",
        "Ensure all necessary directories exist. We'll create:\n",
        "- `data/raw/` - for input CSV files\n",
        "- `data/processed/` - for intermediate processed data\n",
        "\n",
        "Empty directories won't be tracked by git, so we'll add `.gitkeep` files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Create data folders if missing; place a .gitkeep in empty dirs.\n",
        "# Hints:\n",
        "#   - Use pathlib.Path or os.makedirs\n",
        "#   - Check if directory exists before creating\n",
        "#   - Write empty .gitkeep file if directory is empty\n",
        "# Acceptance:\n",
        "#   - data/raw and data/processed exist\n",
        "#   - .gitkeep files are present in empty directories\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure_dirs():\n",
        "    \"\"\"\n",
        "    Create necessary data directories and .gitkeep files.\n",
        "    \n",
        "    Creates:\n",
        "    - data/raw/\n",
        "    - data/processed/\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "ensure_dirs()\n",
        "print(\"Directories ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "With configuration loaded and directories ready, proceed to:\n",
        "1. **01_eda_dataset.ipynb** - Explore and clean your data\n",
        "2. **02_build_hf_dataset.ipynb** - Convert to Hugging Face format\n",
        "3. **03_tokenizer_sanity.ipynb** - Verify tokenization\n",
        "4. **04_eval_harness_cpu.ipynb** - Set up evaluation metrics\n",
        "\n",
        "Then move to GPU notebooks (10, 11) for training.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

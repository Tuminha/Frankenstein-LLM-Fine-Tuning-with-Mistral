{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Base vs Finetuned (GPU)\n",
        "\n",
        "## Comparison Goals\n",
        "\n",
        "After training, we want to:\n",
        "1. **Quantitative:** Compare perplexity (base vs finetuned)\n",
        "2. **Qualitative:** Generate samples side-by-side\n",
        "3. **Document:** Record hyperparameters, costs, latency\n",
        "\n",
        "This notebook runs on GPU for speed, but you can adapt it for CPU if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load base model + attach LoRA adapters; run perplexity on validation slice.\n",
        "# Hints:\n",
        "#   - Load base model in 4-bit\n",
        "#   - Use PeftModel.from_pretrained() to attach adapters\n",
        "#   - Compute perplexity on validation set (similar to notebook 04, but on GPU)\n",
        "# Acceptance:\n",
        "#   - prints ppl_base vs ppl_finetuned\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "def eval_perplexity_with_adapters(base_model: str, adapter_repo: str, tokenized_ds, n_samples: int=500):\n",
        "    \"\"\"\n",
        "    Evaluate perplexity with base and finetuned models.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        tokenized_ds: Tokenized validation dataset\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Evaluate\n",
        "# eval_perplexity_with_adapters(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "#     \"YOURUSER/mistral-frankenstein-qlora\",\n",
        "#     ds_val,\n",
        "#     n_samples=500\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Side-by-Side Generation\n",
        "\n",
        "Generate text with both models using the same prompts. Compare style, coherence, and Frankenstein-like tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generate 3-5 short continuations with both models for side-by-side comparison.\n",
        "# Hints:\n",
        "#   - Load base model and finetuned (base + adapters)\n",
        "#   - Use same prompts for both\n",
        "#   - Print outputs side-by-side or in a table\n",
        "#   - Use reasonable generation parameters (temperature, top_p)\n",
        "# Acceptance:\n",
        "#   - prints paired outputs with fixed prompts\n",
        "\n",
        "def compare_samples(base_model: str, adapter_repo: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate samples with base and finetuned models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Compare\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\",\n",
        "    \"I beheld the wretchâ€”the miserable monster\",\n",
        "    \"Life and death appeared to me ideal bounds\"\n",
        "]\n",
        "# compare_samples(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "#     \"YOURUSER/mistral-frankenstein-qlora\",\n",
        "#     prompts,\n",
        "#     max_new_tokens=100\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

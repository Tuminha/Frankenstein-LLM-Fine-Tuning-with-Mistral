{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Base vs Finetuned (GPU)\n",
        "\n",
        "## Comparison Goals\n",
        "\n",
        "After training, we want to:\n",
        "1. **Quantitative:** Compare perplexity (base vs finetuned)\n",
        "2. **Qualitative:** Generate samples side-by-side\n",
        "3. **Document:** Record hyperparameters, costs, latency\n",
        "\n",
        "This notebook runs on GPU for speed, but you can adapt it for CPU if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  WARNING: No GPU detected! This will be very slow.\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load base model + attach LoRA adapters; run perplexity on validation slice.\n",
        "# Hints:\n",
        "#   - Load base model in 4-bit\n",
        "#   - Use PeftModel.from_pretrained() to attach adapters\n",
        "#   - Compute perplexity on validation set (similar to notebook 04, but on GPU)\n",
        "# Acceptance:\n",
        "#   - prints ppl_base vs ppl_finetuned\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "def eval_perplexity_with_adapters(base_model: str, adapter_repo: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Evaluate perplexity with base and finetuned models.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        dataset: Validation dataset (raw text, not tokenized)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"⚠️  WARNING: No GPU detected! This will be very slow.\")\n",
        "        print(\"   For faster evaluation, enable GPU in Colab: Runtime → Change runtime type → GPU\")\n",
        "        print(\"   Continuing on CPU (this may take 30-60 minutes)...\\n\")\n",
        "    else:\n",
        "        print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Loading base model {base_model}...\")\n",
        "    \n",
        "    # Load tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load base model in 4-bit (same as training)\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "    \n",
        "    # Load model with appropriate device handling\n",
        "    load_kwargs = {\n",
        "        \"quantization_config\": quantization_config,\n",
        "        \"torch_dtype\": torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "    }\n",
        "    if device == \"cuda\":\n",
        "        load_kwargs[\"device_map\"] = \"auto\"\n",
        "    else:\n",
        "        # For CPU, don't use device_map (it's for multi-GPU)\n",
        "        load_kwargs[\"torch_dtype\"] = torch.float32  # bfloat16 not well supported on CPU\n",
        "    \n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        **load_kwargs\n",
        "    )\n",
        "    \n",
        "    if device == \"cpu\":\n",
        "        base_model_obj = base_model_obj.to(\"cpu\")\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    print(\"✅ Base model loaded\")\n",
        "    print(f\"Loading adapters from {adapter_repo}...\")\n",
        "    \n",
        "    # Load finetuned model (base + adapters)\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"✅ Finetuned model loaded\")\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"\\nComputing perplexity on {n_samples} samples...\")\n",
        "    print(\"⚠️  This may take 5-15 minutes. Be patient!\\n\")\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Compute base model perplexity\n",
        "    print(\"Computing BASE model perplexity...\")\n",
        "    base_total_nll = 0.0\n",
        "    base_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = base_model_obj(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            base_total_nll += nll\n",
        "            base_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Base: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    base_avg_nll = base_total_nll / base_total_tokens\n",
        "    base_perplexity = torch.exp(torch.tensor(base_avg_nll)).item()\n",
        "    \n",
        "    # Compute finetuned model perplexity\n",
        "    print(\"\\nComputing FINETUNED model perplexity...\")\n",
        "    finetuned_total_nll = 0.0\n",
        "    finetuned_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = finetuned_model(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            finetuned_total_nll += nll\n",
        "            finetuned_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Finetuned: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    finetuned_avg_nll = finetuned_total_nll / finetuned_total_tokens\n",
        "    finetuned_perplexity = torch.exp(torch.tensor(finetuned_avg_nll)).item()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERPLEXITY COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Base Model:        {base_perplexity:.2f}\")\n",
        "    print(f\"Finetuned Model:   {finetuned_perplexity:.2f}\")\n",
        "    print(f\"Improvement:       {base_perplexity - finetuned_perplexity:.2f} points\")\n",
        "    print(f\"Relative Change:   {((finetuned_perplexity - base_perplexity) / base_perplexity * 100):.1f}%\")\n",
        "    print(f\"\\nSamples evaluated: {n_samples}\")\n",
        "    print(f\"Time taken: {elapsed_time/60:.1f} minutes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return {\n",
        "        'base_perplexity': base_perplexity,\n",
        "        'finetuned_perplexity': finetuned_perplexity,\n",
        "        'improvement': base_perplexity - finetuned_perplexity\n",
        "    }\n",
        "\n",
        "# Load dataset (raw text, not tokenized)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "# Get token (same as notebook 10)\n",
        "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\") or HfFolder.get_token()\n",
        "ds_val = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\", token=hf_token)['validation']\n",
        "\n",
        "# Evaluate\n",
        "results = eval_perplexity_with_adapters(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    ds_val,\n",
        "    n_samples=25  # Start with 25, can increase later\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Side-by-Side Generation\n",
        "\n",
        "Generate text with both models using the same prompts. Compare style, coherence, and Frankenstein-like tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generate 3-5 short continuations with both models for side-by-side comparison.\n",
        "# Hints:\n",
        "#   - Load base model and finetuned (base + adapters)\n",
        "#   - Use same prompts for both\n",
        "#   - Print outputs side-by-side or in a table\n",
        "#   - Use reasonable generation parameters (temperature, top_p)\n",
        "# Acceptance:\n",
        "#   - prints paired outputs with fixed prompts\n",
        "\n",
        "def compare_samples(base_model: str, adapter_repo: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate samples with base and finetuned models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Compare\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\",\n",
        "    \"I beheld the wretch—the miserable monster\",\n",
        "    \"Life and death appeared to me ideal bounds\"\n",
        "]\n",
        "# compare_samples(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "#     \"YOURUSER/mistral-frankenstein-qlora\",\n",
        "#     prompts,\n",
        "#     max_new_tokens=100\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

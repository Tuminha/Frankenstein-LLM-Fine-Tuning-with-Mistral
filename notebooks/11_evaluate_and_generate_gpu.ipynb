{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Base vs Finetuned (GPU)\n",
        "\n",
        "## Comparison Goals\n",
        "\n",
        "After training, we want to:\n",
        "1. **Quantitative:** Compare perplexity (base vs finetuned)\n",
        "2. **Qualitative:** Generate samples side-by-side\n",
        "3. **Document:** Record hyperparameters, costs, latency\n",
        "\n",
        "This notebook runs on GPU for speed, but you can adapt it for CPU if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  WARNING: No GPU detected! This will be very slow.\n",
            "   For faster evaluation, enable GPU in Colab: Runtime → Change runtime type → GPU\n",
            "   Continuing on CPU (this may take 30-60 minutes)...\n",
            "\n",
            "Loading base model mistralai/Mistral-7B-Instruct-v0.2...\n"
          ]
        },
        {
          "ename": "PackageNotFoundError",
          "evalue": "No package metadata was found for bitsandbytes",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/importlib/metadata/__init__.py:563\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m.discover(name=name))\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[31mStopIteration\u001b[39m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    163\u001b[39m ds_val = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mTuminha/frankenstein-fanfic-snippets\u001b[39m\u001b[33m\"\u001b[39m, token=hf_token)[\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m results = \u001b[43meval_perplexity_with_adapters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistralai/Mistral-7B-Instruct-v0.2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTuminha/mistral-frankenstein-qlora\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with 25, can increase later\u001b[39;49;00m\n\u001b[32m    171\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36meval_perplexity_with_adapters\u001b[39m\u001b[34m(base_model, adapter_repo, dataset, n_samples)\u001b[39m\n\u001b[32m     36\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Load base model in 4-bit (same as training)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m quantization_config = \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnf4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Load model with appropriate device handling\u001b[39;00m\n\u001b[32m     46\u001b[39m load_kwargs = {\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m: quantization_config,\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m: torch.bfloat16 \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.float32\n\u001b[32m     49\u001b[39m }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/utils/quantization_config.py:510\u001b[39m, in \u001b[36mBitsAndBytesConfig.__init__\u001b[39m\u001b[34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    508\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/utils/quantization_config.py:568\u001b[39m, in \u001b[36mBitsAndBytesConfig.post_init\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.bnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version.parse(\u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbitsandbytes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) >= version.parse(\n\u001b[32m    569\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m0.39.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m ):\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    572\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    573\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/importlib/metadata/__init__.py:1008\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(distribution_name):\n\u001b[32m   1002\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m   1003\u001b[39m \n\u001b[32m   1004\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[33;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[33;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[32m   1007\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m.version\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/importlib/metadata/__init__.py:981\u001b[39m, in \u001b[36mdistribution\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistribution\u001b[39m(distribution_name):\n\u001b[32m    976\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[32m    977\u001b[39m \n\u001b[32m    978\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[33;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/importlib/metadata/__init__.py:565\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m.discover(name=name))\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
            "\u001b[31mPackageNotFoundError\u001b[39m: No package metadata was found for bitsandbytes"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load base model + attach LoRA adapters; run perplexity on validation slice.\n",
        "# Hints:\n",
        "#   - Load base model in 4-bit\n",
        "#   - Use PeftModel.from_pretrained() to attach adapters\n",
        "#   - Compute perplexity on validation set (similar to notebook 04, but on GPU)\n",
        "# Acceptance:\n",
        "#   - prints ppl_base vs ppl_finetuned\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "def eval_perplexity_with_adapters(base_model: str, adapter_repo: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Evaluate perplexity with base and finetuned models.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        dataset: Validation dataset (raw text, not tokenized)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"⚠️  WARNING: No GPU detected! This will be very slow.\")\n",
        "        print(\"   For faster evaluation, enable GPU in Colab: Runtime → Change runtime type → GPU\")\n",
        "        print(\"   Continuing on CPU (this may take 30-60 minutes)...\\n\")\n",
        "    else:\n",
        "        print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Loading base model {base_model}...\")\n",
        "    \n",
        "    # Load tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Check if bitsandbytes is available (required for 4-bit quantization)\n",
        "    try:\n",
        "        import bitsandbytes\n",
        "        use_quantization = device == \"cuda\"  # Only use quantization on GPU\n",
        "    except ImportError:\n",
        "        use_quantization = False\n",
        "        if device == \"cuda\":\n",
        "            print(\"⚠️  bitsandbytes not installed. Loading model without quantization (will use more memory).\")\n",
        "        else:\n",
        "            print(\"ℹ️  bitsandbytes not available on CPU. Loading model without quantization.\")\n",
        "    \n",
        "    # Load base model with or without quantization\n",
        "    load_kwargs = {}\n",
        "    \n",
        "    if use_quantization:\n",
        "        # Use 4-bit quantization (same as training) - GPU only\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "        load_kwargs[\"quantization_config\"] = quantization_config\n",
        "        load_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
        "        load_kwargs[\"device_map\"] = \"auto\"\n",
        "        print(\"Loading model with 4-bit quantization...\")\n",
        "    else:\n",
        "        # Load without quantization (CPU or if bitsandbytes unavailable)\n",
        "        if device == \"cuda\":\n",
        "            load_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
        "            load_kwargs[\"device_map\"] = \"auto\"\n",
        "        else:\n",
        "            load_kwargs[\"torch_dtype\"] = torch.float32\n",
        "        print(\"Loading model without quantization...\")\n",
        "    \n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        **load_kwargs\n",
        "    )\n",
        "    \n",
        "    if device == \"cpu\" and not use_quantization:\n",
        "        base_model_obj = base_model_obj.to(\"cpu\")\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    print(\"✅ Base model loaded\")\n",
        "    print(f\"Loading adapters from {adapter_repo}...\")\n",
        "    \n",
        "    # Load finetuned model (base + adapters)\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"✅ Finetuned model loaded\")\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"\\nComputing perplexity on {n_samples} samples...\")\n",
        "    print(\"⚠️  This may take 5-15 minutes. Be patient!\\n\")\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Compute base model perplexity\n",
        "    print(\"Computing BASE model perplexity...\")\n",
        "    base_total_nll = 0.0\n",
        "    base_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = base_model_obj(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            base_total_nll += nll\n",
        "            base_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Base: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    base_avg_nll = base_total_nll / base_total_tokens\n",
        "    base_perplexity = torch.exp(torch.tensor(base_avg_nll)).item()\n",
        "    \n",
        "    # Compute finetuned model perplexity\n",
        "    print(\"\\nComputing FINETUNED model perplexity...\")\n",
        "    finetuned_total_nll = 0.0\n",
        "    finetuned_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = finetuned_model(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            finetuned_total_nll += nll\n",
        "            finetuned_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Finetuned: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    finetuned_avg_nll = finetuned_total_nll / finetuned_total_tokens\n",
        "    finetuned_perplexity = torch.exp(torch.tensor(finetuned_avg_nll)).item()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERPLEXITY COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Base Model:        {base_perplexity:.2f}\")\n",
        "    print(f\"Finetuned Model:   {finetuned_perplexity:.2f}\")\n",
        "    print(f\"Improvement:       {base_perplexity - finetuned_perplexity:.2f} points\")\n",
        "    print(f\"Relative Change:   {((finetuned_perplexity - base_perplexity) / base_perplexity * 100):.1f}%\")\n",
        "    print(f\"\\nSamples evaluated: {n_samples}\")\n",
        "    print(f\"Time taken: {elapsed_time/60:.1f} minutes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return {\n",
        "        'base_perplexity': base_perplexity,\n",
        "        'finetuned_perplexity': finetuned_perplexity,\n",
        "        'improvement': base_perplexity - finetuned_perplexity\n",
        "    }\n",
        "\n",
        "# Load dataset (raw text, not tokenized)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "# Get token (same as notebook 10)\n",
        "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\") or HfFolder.get_token()\n",
        "ds_val = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\", token=hf_token)['validation']\n",
        "\n",
        "# Evaluate\n",
        "results = eval_perplexity_with_adapters(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    ds_val,\n",
        "    n_samples=25  # Start with 25, can increase later\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Side-by-Side Generation\n",
        "\n",
        "Generate text with both models using the same prompts. Compare style, coherence, and Frankenstein-like tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generate 3-5 short continuations with both models for side-by-side comparison.\n",
        "# Hints:\n",
        "#   - Load base model and finetuned (base + adapters)\n",
        "#   - Use same prompts for both\n",
        "#   - Print outputs side-by-side or in a table\n",
        "#   - Use reasonable generation parameters (temperature, top_p)\n",
        "# Acceptance:\n",
        "#   - prints paired outputs with fixed prompts\n",
        "\n",
        "def compare_samples(base_model: str, adapter_repo: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate samples with base and finetuned models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Compare\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\",\n",
        "    \"I beheld the wretch—the miserable monster\",\n",
        "    \"Life and death appeared to me ideal bounds\"\n",
        "]\n",
        "# compare_samples(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "#     \"YOURUSER/mistral-frankenstein-qlora\",\n",
        "#     prompts,\n",
        "#     max_new_tokens=100\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

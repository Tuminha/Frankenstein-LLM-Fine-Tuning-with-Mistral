{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Base vs Finetuned (GPU)\n",
        "\n",
        "## Comparison Goals\n",
        "\n",
        "After training, we want to:\n",
        "1. **Quantitative:** Compare perplexity (base vs finetuned)\n",
        "2. **Qualitative:** Generate samples side-by-side\n",
        "3. **Document:** Record hyperparameters, costs, latency\n",
        "\n",
        "This notebook runs on GPU for speed, but you can adapt it for CPU if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m138.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.3.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# === INSTALL DEPENDENCIES (Run this first!) ===\n",
        "# CRITICAL: This notebook requires Python 3.10 or 3.11 (NOT 3.12)\n",
        "# bitsandbytes does not have wheels for Python 3.12 yet\n",
        "\n",
        "import sys\n",
        "\n",
        "# Check Python version\n",
        "if sys.version_info >= (3, 12):\n",
        "    raise SystemExit(\n",
        "        \"âŒ Python 3.12 detected. bitsandbytes does not publish wheels for 3.12 yet.\\n\"\n",
        "        \"   SOLUTION: In Colab, go to Runtime â†’ Change runtime type â†’ Python version â†’ Select 3.11\\n\"\n",
        "        \"   Then restart the runtime and rerun this cell.\"\n",
        "    )\n",
        "elif sys.version_info < (3, 10):\n",
        "    raise SystemExit(\n",
        "        \"âŒ Python version too old. Need Python 3.10 or 3.11.\\n\"\n",
        "        \"   In Colab: Runtime â†’ Change runtime type â†’ Python version â†’ Select 3.11\"\n",
        "    )\n",
        "\n",
        "print(f\"âœ… Python {sys.version_info.major}.{sys.version_info.minor} detected (compatible)\")\n",
        "\n",
        "# Install packages with compatible versions\n",
        "# NOTE: We pin triton to avoid the triton.ops compatibility issue\n",
        "print(\"\\nğŸ“¦ Installing packages (this may take 2-3 minutes)...\")\n",
        "\n",
        "%pip install -q \"torch>=2.3.0,<2.5.0\" \"transformers>=4.39.0,<4.45\" \\\n",
        "  \"peft>=0.10.0\" \"accelerate>=0.30.0\" \"datasets>=2.20.0\" \\\n",
        "  \"triton>=2.1.0,<2.2.0\" \"bitsandbytes>=0.43.0,<0.44.0\" \\\n",
        "  \"trl\" \"huggingface_hub\" \"packaging\"\n",
        "\n",
        "print(\"\\nâœ… Installation complete! Verifying packages...\")\n",
        "\n",
        "# Verify installation\n",
        "try:\n",
        "    import torch\n",
        "    import transformers\n",
        "    import datasets\n",
        "    import peft\n",
        "    import accelerate\n",
        "    import triton\n",
        "    import bitsandbytes\n",
        "    import trl\n",
        "    import huggingface_hub\n",
        "    from packaging import version\n",
        "    import importlib.metadata\n",
        "    \n",
        "    print(\"\\nâœ… All packages installed successfully:\")\n",
        "    print(f\"   - Python: {sys.version.split()[0]}\")\n",
        "    print(f\"   - torch: {torch.__version__}\")\n",
        "    print(f\"   - transformers: {transformers.__version__}\")\n",
        "    print(f\"   - datasets: {datasets.__version__}\")\n",
        "    print(f\"   - peft: {peft.__version__}\")\n",
        "    print(f\"   - accelerate: {accelerate.__version__}\")\n",
        "    print(f\"   - triton: {triton.__version__}\")\n",
        "    print(f\"   - bitsandbytes: {importlib.metadata.version('bitsandbytes')}\")\n",
        "    print(f\"   - trl: {trl.__version__}\")\n",
        "    \n",
        "    # Test bitsandbytes import (this is where the error usually occurs)\n",
        "    print(\"\\nğŸ” Testing bitsandbytes import...\")\n",
        "    try:\n",
        "        from bitsandbytes.nn import Linear4bit\n",
        "        print(\"âœ… bitsandbytes imports successfully!\")\n",
        "    except ImportError as e:\n",
        "        if \"triton.ops\" in str(e):\n",
        "            print(\"âš ï¸  WARNING: triton.ops compatibility issue detected\")\n",
        "            print(\"   Attempting fix: reinstalling compatible triton version...\")\n",
        "            %pip install -q --force-reinstall \"triton==2.1.0\"\n",
        "            print(\"   Please RESTART THE RUNTIME (Runtime â†’ Restart runtime)\")\n",
        "            print(\"   Then rerun this cell.\")\n",
        "            raise SystemExit(\"Runtime restart required\")\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    print(\"\\nâœ… All checks passed! You can proceed to the next cell.\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"\\nâŒ Import error: {e}\")\n",
        "    if \"triton.ops\" in str(e):\n",
        "        print(\"\\nğŸ”§ FIX: The triton.ops module is missing.\")\n",
        "        print(\"   This is a compatibility issue between triton and bitsandbytes.\")\n",
        "        print(\"   SOLUTION:\")\n",
        "        print(\"   1. Restart the runtime: Runtime â†’ Restart runtime\")\n",
        "        print(\"   2. Rerun this cell\")\n",
        "        print(\"   3. If it still fails, try: %pip install --force-reinstall triton==2.1.0\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Unexpected error: {e}\")\n",
        "    print(\"   Please check the error message above and try:\")\n",
        "    print(\"   1. Restart the runtime: Runtime â†’ Restart runtime\")\n",
        "    print(\"   2. Rerun this cell\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPU detected: Tesla T4\n",
            "Loading base model mistralai/Mistral-7B-Instruct-v0.2...\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "âŒ bitsandbytes is REQUIRED to load QLoRA adapters!\n   The adapters were trained with 4-bit quantization and require the same model structure.\n   Install it with: !pip install -U bitsandbytes\n   Then restart the runtime and run this cell again.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1220812399.py\u001b[0m in \u001b[0;36meval_perplexity_with_adapters\u001b[0;34m(base_model, adapter_repo, dataset, n_samples)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m from .triton_based_modules import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mStandardLinear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/triton_based_modules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_rowwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdequantize_rowwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from bitsandbytes.triton.int8_matmul_mixed_dequantize import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mint8_matmul_mixed_dequantize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_perf_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mearly_config_prune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate_matmul_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'triton.ops'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1220812399.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m results = eval_perplexity_with_adapters(\n\u001b[0m\u001b[1;32m    291\u001b[0m     \u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;34m\"Tuminha/mistral-frankenstein-qlora\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1220812399.py\u001b[0m in \u001b[0;36meval_perplexity_with_adapters\u001b[0;34m(base_model, adapter_repo, dataset, n_samples)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m     69\u001b[0m                     \u001b[0;34m\"âŒ bitsandbytes is REQUIRED to load QLoRA adapters!\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;34m\"   The adapters were trained with 4-bit quantization and require the same model structure.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: âŒ bitsandbytes is REQUIRED to load QLoRA adapters!\n   The adapters were trained with 4-bit quantization and require the same model structure.\n   Install it with: !pip install -U bitsandbytes\n   Then restart the runtime and run this cell again.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load base model + attach LoRA adapters; run perplexity on validation slice.\n",
        "# Hints:\n",
        "#   - Load base model in 4-bit\n",
        "#   - Use PeftModel.from_pretrained() to attach adapters\n",
        "#   - Compute perplexity on validation set (similar to notebook 04, but on GPU)\n",
        "# Acceptance:\n",
        "#   - prints ppl_base vs ppl_finetuned\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "def eval_perplexity_with_adapters(base_model: str, adapter_repo: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Evaluate perplexity with base and finetuned models.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        dataset: Validation dataset (raw text, not tokenized)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"âš ï¸  WARNING: No GPU detected! This will be very slow.\")\n",
        "        print(\"   For faster evaluation, enable GPU in Colab: Runtime â†’ Change runtime type â†’ GPU\")\n",
        "        print(\"   Continuing on CPU (this may take 30-60 minutes)...\\n\")\n",
        "    else:\n",
        "        print(f\"âœ… GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Loading base model {base_model}...\")\n",
        "    \n",
        "    # Load tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # CRITICAL: Adapters were trained with 4-bit quantization, so we MUST use the same structure\n",
        "    # Check if bitsandbytes is available and up-to-date (REQUIRED for loading adapters trained with QLoRA)\n",
        "    try:\n",
        "        import bitsandbytes\n",
        "        from packaging import version\n",
        "        import importlib.metadata\n",
        "        \n",
        "        # Check version - need >= 0.42.0 for 4-bit quantization\n",
        "        bnb_version = importlib.metadata.version(\"bitsandbytes\")\n",
        "        min_version = version.parse(\"0.42.0\")\n",
        "        current_version = version.parse(bnb_version)\n",
        "        \n",
        "        if current_version < min_version:\n",
        "            raise ImportError(\n",
        "                f\"âŒ bitsandbytes version {bnb_version} is too old! Need >= 0.42.0\\n\"\n",
        "                \"   Upgrade it with: %pip install -U bitsandbytes\\n\"\n",
        "                \"   Then restart the runtime and run this cell again.\"\n",
        "            )\n",
        "        \n",
        "        use_quantization = device == \"cuda\"  # Only use quantization on GPU\n",
        "        print(f\"âœ… bitsandbytes {bnb_version} is installed and compatible\")\n",
        "        \n",
        "    except ImportError as e:\n",
        "        use_quantization = False\n",
        "        if device == \"cuda\":\n",
        "            error_msg = str(e)\n",
        "            \n",
        "            # Check for triton.ops compatibility issue (most common problem)\n",
        "            if \"triton.ops\" in error_msg or \"No module named 'triton.ops'\" in error_msg:\n",
        "                raise ImportError(\n",
        "                    \"âŒ TRITON COMPATIBILITY ISSUE DETECTED!\\n\\n\"\n",
        "                    \"   The error 'No module named triton.ops' means bitsandbytes and triton versions are incompatible.\\n\\n\"\n",
        "                    \"   SOLUTION:\\n\"\n",
        "                    \"   1. Go back to Cell 1 and rerun it (it should fix this automatically)\\n\"\n",
        "                    \"   2. If that doesn't work, restart the runtime: Runtime â†’ Restart runtime\\n\"\n",
        "                    \"   3. Then rerun Cell 1, then this cell\\n\"\n",
        "                    \"   4. If still failing, manually run:\\n\"\n",
        "                    \"      %pip install --force-reinstall triton==2.1.0\\n\"\n",
        "                    \"      Then restart runtime and rerun cells\\n\\n\"\n",
        "                    \"   This is a known compatibility issue between bitsandbytes and newer triton versions.\"\n",
        "                ) from e\n",
        "            elif \"version\" in error_msg.lower() or \"0.39\" in error_msg:\n",
        "                # Version issue - already handled above\n",
        "                raise\n",
        "            else:\n",
        "                raise ImportError(\n",
        "                    \"âŒ bitsandbytes is REQUIRED to load QLoRA adapters!\\n\"\n",
        "                    \"   The adapters were trained with 4-bit quantization and require the same model structure.\\n\\n\"\n",
        "                    f\"   Error details: {error_msg}\\n\\n\"\n",
        "                    \"   SOLUTION:\\n\"\n",
        "                    \"   1. Make sure you ran Cell 1 (installation cell) first\\n\"\n",
        "                    \"   2. Restart the runtime: Runtime â†’ Restart runtime\\n\"\n",
        "                    \"   3. Rerun Cell 1, then this cell\\n\"\n",
        "                    \"   4. If Python 3.12: Change to Python 3.11 in Runtime â†’ Change runtime type\"\n",
        "                ) from e\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                \"âŒ Cannot load QLoRA adapters on CPU!\\n\"\n",
        "                \"   The adapters were trained with 4-bit quantization (GPU only).\\n\"\n",
        "                \"   Please run this notebook on GPU (Colab: Runtime â†’ Change runtime type â†’ GPU).\"\n",
        "            )\n",
        "    \n",
        "    # Load base model - MUST use 4-bit quantization to match adapter structure\n",
        "    load_kwargs = {}\n",
        "    \n",
        "    if use_quantization:\n",
        "        # Use 4-bit quantization (same as training) - REQUIRED for adapter compatibility\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "        load_kwargs[\"quantization_config\"] = quantization_config\n",
        "        load_kwargs[\"dtype\"] = torch.bfloat16\n",
        "        load_kwargs[\"device_map\"] = \"auto\"\n",
        "        print(\"Loading model with 4-bit quantization (required for adapter compatibility)...\")\n",
        "    else:\n",
        "        # This should not happen if checks above work, but just in case\n",
        "        raise RuntimeError(\"Quantization is required but not available!\")\n",
        "    \n",
        "    # Load base model with quantization (required for adapter compatibility)\n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        **load_kwargs\n",
        "    )\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    print(\"âœ… Base model loaded\")\n",
        "    print(f\"Loading adapters from {adapter_repo}...\")\n",
        "    \n",
        "    # Load finetuned model (base + adapters)\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    \n",
        "    # CRITICAL: For 4-bit quantized models, we may need to merge adapters for inference\n",
        "    # Try merging adapters to ensure they're active (this is safe for inference)\n",
        "    try:\n",
        "        # Check if we can merge (some PEFT versions support this)\n",
        "        if hasattr(finetuned_model, 'merge_and_unload'):\n",
        "            print(\"âš ï¸  Attempting to merge adapters for better inference performance...\")\n",
        "            # Note: merge_and_unload() may not work with 4-bit, so we'll try-catch it\n",
        "            try:\n",
        "                finetuned_model = finetuned_model.merge_and_unload()\n",
        "                print(\"âœ… Adapters merged successfully\")\n",
        "            except Exception as merge_error:\n",
        "                print(f\"âš ï¸  Could not merge adapters (expected with 4-bit): {merge_error}\")\n",
        "                print(\"   Continuing with unmerged adapters (should still work)\")\n",
        "        else:\n",
        "            # Ensure adapters are active\n",
        "            if hasattr(finetuned_model, 'set_adapter'):\n",
        "                finetuned_model.set_adapter('default')\n",
        "                print(\"âœ… Adapters set to 'default'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Note: {e}\")\n",
        "        print(\"   Continuing with adapters as loaded...\")\n",
        "    \n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"âœ… Finetuned model loaded\")\n",
        "    \n",
        "    # DIAGNOSTIC: Verify adapters are actually loaded and active\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ADAPTER DIAGNOSTICS\")\n",
        "    print(\"=\"*60)\n",
        "    try:\n",
        "        # Check if adapters are present\n",
        "        if hasattr(finetuned_model, 'peft_config'):\n",
        "            print(f\"âœ… Adapters found: {list(finetuned_model.peft_config.keys())}\")\n",
        "        else:\n",
        "            print(\"âš ï¸  WARNING: No peft_config found - adapters may not be loaded!\")\n",
        "        \n",
        "        # Check active adapters\n",
        "        if hasattr(finetuned_model, 'active_adapters'):\n",
        "            active = finetuned_model.active_adapters\n",
        "            print(f\"âœ… Active adapters: {active}\")\n",
        "        else:\n",
        "            print(\"âš ï¸  WARNING: Cannot check active adapters\")\n",
        "        \n",
        "        # Check adapter parameters\n",
        "        trainable_params = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in finetuned_model.parameters())\n",
        "        print(f\"âœ… Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "        \n",
        "        # Compare a single forward pass to see if outputs differ\n",
        "        test_text = \"It was on a dreary night of November that\"\n",
        "        test_input = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
        "        if device == \"cuda\":\n",
        "            test_input = {k: v.to(device) for k, v in test_input.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            base_output = base_model_obj(**test_input)\n",
        "            finetuned_output = finetuned_model(**test_input)\n",
        "        \n",
        "        base_logits = base_output.logits[0, -1, :10].cpu()  # First 10 logits of last token\n",
        "        finetuned_logits = finetuned_output.logits[0, -1, :10].cpu()\n",
        "        \n",
        "        logit_diff = torch.abs(base_logits - finetuned_logits).mean().item()\n",
        "        print(f\"âœ… Logit difference (first 10 tokens): {logit_diff:.4f}\")\n",
        "        if logit_diff < 0.001:\n",
        "            print(\"âš ï¸  WARNING: Logits are nearly identical! Adapters may not be active.\")\n",
        "            print(\"   Possible causes:\")\n",
        "            print(\"   1. Adapters didn't learn meaningful changes during training\")\n",
        "            print(\"   2. Training loss reduction was minimal (check training logs)\")\n",
        "            print(\"   3. 4-bit quantization compatibility issue with PEFT version\")\n",
        "            print(\"   4. Adapters need to be explicitly enabled (try set_adapter if available)\")\n",
        "            print(\"   Recommendation: Check training logs to verify loss decreased during training\")\n",
        "        else:\n",
        "            print(\"âœ… Logits differ - adapters appear to be active\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Error during diagnostics: {e}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"\\nComputing perplexity on {n_samples} samples...\")\n",
        "    print(\"âš ï¸  This may take 5-15 minutes. Be patient!\\n\")\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Compute base model perplexity\n",
        "    print(\"Computing BASE model perplexity...\")\n",
        "    base_total_nll = 0.0\n",
        "    base_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = base_model_obj(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            base_total_nll += nll\n",
        "            base_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Base: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    base_avg_nll = base_total_nll / base_total_tokens\n",
        "    base_perplexity = torch.exp(torch.tensor(base_avg_nll)).item()\n",
        "    \n",
        "    # Compute finetuned model perplexity\n",
        "    print(\"\\nComputing FINETUNED model perplexity...\")\n",
        "    finetuned_total_nll = 0.0\n",
        "    finetuned_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = finetuned_model(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            finetuned_total_nll += nll\n",
        "            finetuned_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Finetuned: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    finetuned_avg_nll = finetuned_total_nll / finetuned_total_tokens\n",
        "    finetuned_perplexity = torch.exp(torch.tensor(finetuned_avg_nll)).item()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERPLEXITY COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Base Model:        {base_perplexity:.2f}\")\n",
        "    print(f\"Finetuned Model:   {finetuned_perplexity:.2f}\")\n",
        "    print(f\"Improvement:       {base_perplexity - finetuned_perplexity:.2f} points\")\n",
        "    print(f\"Relative Change:   {((finetuned_perplexity - base_perplexity) / base_perplexity * 100):.1f}%\")\n",
        "    print(f\"\\nSamples evaluated: {n_samples}\")\n",
        "    print(f\"Time taken: {elapsed_time/60:.1f} minutes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return {\n",
        "        'base_perplexity': base_perplexity,\n",
        "        'finetuned_perplexity': finetuned_perplexity,\n",
        "        'improvement': base_perplexity - finetuned_perplexity\n",
        "    }\n",
        "\n",
        "# Load dataset (raw text, not tokenized)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "# Get token - use environment variable or Hugging Face login\n",
        "# Option 1: Set HF_TOKEN environment variable\n",
        "# Option 2: Use: from huggingface_hub import login; login()\n",
        "hf_token = os.getenv(\"HF_TOKEN\") or HfFolder.get_token()\n",
        "if not hf_token:\n",
        "    print(\"âš ï¸  WARNING: No HF token found. Set HF_TOKEN environment variable or use login()\")\n",
        "    print(\"   For Colab: Use Colab secrets (HF_TOKEN) or login()\")\n",
        "    hf_token = None  # Will try without token (may fail for private datasets)\n",
        "\n",
        "ds_val = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\", token=hf_token)['validation']\n",
        "\n",
        "# Evaluate\n",
        "results = eval_perplexity_with_adapters(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    ds_val,\n",
        "    n_samples=25  # Start with 25, can increase later\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Side-by-Side Generation\n",
        "\n",
        "Generate text with both models using the same prompts. Compare style, coherence, and Frankenstein-like tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generate 3-5 short continuations with both models for side-by-side comparison.\n",
        "# Hints:\n",
        "#   - Load base model and finetuned (base + adapters)\n",
        "#   - Use same prompts for both\n",
        "#   - Print outputs side-by-side or in a table\n",
        "#   - Use reasonable generation parameters (temperature, top_p)\n",
        "# Acceptance:\n",
        "#   - prints paired outputs with fixed prompts\n",
        "\n",
        "def compare_samples(base_model: str, adapter_repo: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate samples with base and finetuned models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "    from peft import PeftModel\n",
        "    import torch\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    print(\"Loading models for generation comparison...\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load base model (same as in perplexity evaluation)\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "    \n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quantization_config,\n",
        "        dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    # Load finetuned model\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"âœ… Models loaded for generation\\n\")\n",
        "    \n",
        "    # Generation parameters\n",
        "    generation_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "        \"do_sample\": True,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"SIDE-BY-SIDE GENERATION COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PROMPT {i}: {prompt}\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Tokenize prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if device == \"cuda\":\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate with base model\n",
        "        print(\"\\nğŸ“˜ BASE MODEL:\")\n",
        "        print(\"-\" * 80)\n",
        "        with torch.no_grad():\n",
        "            base_outputs = base_model_obj.generate(**inputs, **generation_kwargs)\n",
        "        base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "        # Only show the generated part (after the prompt)\n",
        "        base_generated = base_text[len(prompt):].strip()\n",
        "        print(base_generated)\n",
        "        \n",
        "        # Generate with finetuned model\n",
        "        print(\"\\nğŸ“— FINETUNED MODEL:\")\n",
        "        print(\"-\" * 80)\n",
        "        with torch.no_grad():\n",
        "            finetuned_outputs = finetuned_model.generate(**inputs, **generation_kwargs)\n",
        "        finetuned_text = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "        finetuned_generated = finetuned_text[len(prompt):].strip()\n",
        "        print(finetuned_generated)\n",
        "        \n",
        "        # Highlight differences (simple comparison)\n",
        "        if base_generated != finetuned_generated:\n",
        "            print(\"\\nâœ… Outputs differ - adapters are affecting generation\")\n",
        "        else:\n",
        "            print(\"\\nâš ï¸  Outputs are identical - adapters may not be active\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# Compare\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\",\n",
        "    \"I beheld the wretchâ€”the miserable monster\",\n",
        "    \"Life and death appeared to me ideal bounds\"\n",
        "]\n",
        "compare_samples(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    prompts,\n",
        "    max_new_tokens=100\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

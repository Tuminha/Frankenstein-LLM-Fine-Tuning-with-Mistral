{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Base vs Finetuned (GPU)\n",
        "\n",
        "## Comparison Goals\n",
        "\n",
        "After training, we want to:\n",
        "1. **Quantitative:** Compare perplexity (base vs finetuned)\n",
        "2. **Qualitative:** Generate samples side-by-side\n",
        "3. **Document:** Record hyperparameters, costs, latency\n",
        "\n",
        "This notebook runs on GPU for speed, but you can adapt it for CPU if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "SystemExit",
          "evalue": "‚ùå Python 3.12 detected. bitsandbytes does not publish wheels for 3.12 yet.\n   Switch the runtime/kernel to Python 3.10 or 3.11, then rerun this cell.",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m ‚ùå Python 3.12 detected. bitsandbytes does not publish wheels for 3.12 yet.\n   Switch the runtime/kernel to Python 3.10 or 3.11, then rerun this cell.\n"
          ]
        }
      ],
      "source": [
        "# === INSTALL DEPENDENCIES (Run this first!) ===\n",
        "# Install all required packages from requirements-gpu.txt\n",
        "# These are needed for loading and evaluating the finetuned model with QLoRA adapters\n",
        "\n",
        "import sys\n",
        "\n",
        "if sys.version_info >= (3, 12):\n",
        "    raise SystemExit(\n",
        "        \"‚ùå Python 3.12 detected. bitsandbytes does not publish wheels for 3.12 yet.\\n\"\n",
        "        \"   Switch the runtime/kernel to Python 3.10 or 3.11, then rerun this cell.\"\n",
        "    )\n",
        "\n",
        "%pip install -U torch \"transformers<4.45\" datasets peft \"accelerate>=0.27\" \"bitsandbytes>=0.42.0\" trl huggingface_hub\n",
        "\n",
        "# Verify installation\n",
        "try:\n",
        "    import torch\n",
        "    import transformers\n",
        "    import datasets\n",
        "    import peft\n",
        "    import accelerate\n",
        "    import bitsandbytes\n",
        "    import trl\n",
        "    import huggingface_hub\n",
        "    from packaging import version\n",
        "    import importlib.metadata\n",
        "    \n",
        "    print(\"‚úÖ All packages installed successfully:\")\n",
        "    print(f\"   - python: {sys.version.split()[0]}\")\n",
        "    print(f\"   - torch: {torch.__version__}\")\n",
        "    print(f\"   - transformers: {transformers.__version__}\")\n",
        "    print(f\"   - datasets: {datasets.__version__}\")\n",
        "    print(f\"   - peft: {peft.__version__}\")\n",
        "    print(f\"   - accelerate: {accelerate.__version__}\")\n",
        "    print(f\"   - bitsandbytes: {importlib.metadata.version('bitsandbytes')}\")\n",
        "    print(f\"   - trl: {trl.__version__}\")\n",
        "    print(f\"   - huggingface_hub: {huggingface_hub.__version__}\")\n",
        "    \n",
        "    # Check critical version requirements\n",
        "    bnb_version = importlib.metadata.version(\"bitsandbytes\")\n",
        "    if version.parse(bnb_version) < version.parse(\"0.42.0\"):\n",
        "        print(\"‚ö†Ô∏è  WARNING: bitsandbytes version is < 0.42.0. You may need to restart the runtime.\")\n",
        "        print(\"   Upgrade with: pip install -U bitsandbytes  # then restart kernel\")\n",
        "    \n",
        "    transformers_version = transformers.__version__\n",
        "    if version.parse(transformers_version) >= version.parse(\"4.45\"):\n",
        "        print(\"‚ö†Ô∏è  WARNING: transformers version is >= 4.45. Requirements specify <4.45.\")\n",
        "    \n",
        "    accelerate_version = accelerate.__version__\n",
        "    if version.parse(accelerate_version) < version.parse(\"0.27\"):\n",
        "        print(\"‚ö†Ô∏è  WARNING: accelerate version is < 0.27. Requirements specify >=0.27.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error checking packages: {e}\")\n",
        "    print(\"   Please restart the runtime after installation and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU detected: Tesla T4\n",
            "Loading base model mistralai/Mistral-7B-Instruct-v0.2...\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "‚ùå bitsandbytes is REQUIRED to load QLoRA adapters!\n   The adapters were trained with 4-bit quantization and require the same model structure.\n   Install it with: !pip install -U bitsandbytes\n   Then restart the runtime and run this cell again.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1220812399.py\u001b[0m in \u001b[0;36meval_perplexity_with_adapters\u001b[0;34m(base_model, adapter_repo, dataset, n_samples)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1220812399.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m results = eval_perplexity_with_adapters(\n\u001b[0m\u001b[1;32m    291\u001b[0m     \u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;34m\"Tuminha/mistral-frankenstein-qlora\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1220812399.py\u001b[0m in \u001b[0;36meval_perplexity_with_adapters\u001b[0;34m(base_model, adapter_repo, dataset, n_samples)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m     69\u001b[0m                     \u001b[0;34m\"‚ùå bitsandbytes is REQUIRED to load QLoRA adapters!\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;34m\"   The adapters were trained with 4-bit quantization and require the same model structure.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: ‚ùå bitsandbytes is REQUIRED to load QLoRA adapters!\n   The adapters were trained with 4-bit quantization and require the same model structure.\n   Install it with: !pip install -U bitsandbytes\n   Then restart the runtime and run this cell again.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load base model + attach LoRA adapters; run perplexity on validation slice.\n",
        "# Hints:\n",
        "#   - Load base model in 4-bit\n",
        "#   - Use PeftModel.from_pretrained() to attach adapters\n",
        "#   - Compute perplexity on validation set (similar to notebook 04, but on GPU)\n",
        "# Acceptance:\n",
        "#   - prints ppl_base vs ppl_finetuned\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "def eval_perplexity_with_adapters(base_model: str, adapter_repo: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Evaluate perplexity with base and finetuned models.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        dataset: Validation dataset (raw text, not tokenized)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow.\")\n",
        "        print(\"   For faster evaluation, enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "        print(\"   Continuing on CPU (this may take 30-60 minutes)...\\n\")\n",
        "    else:\n",
        "        print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Loading base model {base_model}...\")\n",
        "    \n",
        "    # Load tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # CRITICAL: Adapters were trained with 4-bit quantization, so we MUST use the same structure\n",
        "    # Check if bitsandbytes is available and up-to-date (REQUIRED for loading adapters trained with QLoRA)\n",
        "    try:\n",
        "        import bitsandbytes\n",
        "        from packaging import version\n",
        "        import importlib.metadata\n",
        "        \n",
        "        # Check version - need >= 0.42.0 for 4-bit quantization\n",
        "        bnb_version = importlib.metadata.version(\"bitsandbytes\")\n",
        "        min_version = version.parse(\"0.42.0\")\n",
        "        current_version = version.parse(bnb_version)\n",
        "        \n",
        "        if current_version < min_version:\n",
        "            raise ImportError(\n",
        "                f\"‚ùå bitsandbytes version {bnb_version} is too old! Need >= 0.42.0\\n\"\n",
        "                \"   Upgrade it with: !pip install -U bitsandbytes\\n\"\n",
        "                \"   Then restart the runtime and run this cell again.\"\n",
        "            )\n",
        "        \n",
        "        use_quantization = device == \"cuda\"  # Only use quantization on GPU\n",
        "        print(f\"‚úÖ bitsandbytes {bnb_version} is installed and compatible\")\n",
        "        \n",
        "    except ImportError as e:\n",
        "        use_quantization = False\n",
        "        if device == \"cuda\":\n",
        "            error_msg = str(e)\n",
        "            if \"version\" in error_msg.lower() or \"0.39\" in error_msg:\n",
        "                # Version issue - already handled above\n",
        "                raise\n",
        "            else:\n",
        "                raise ImportError(\n",
        "                    \"‚ùå bitsandbytes is REQUIRED to load QLoRA adapters!\\n\"\n",
        "                    \"   The adapters were trained with 4-bit quantization and require the same model structure.\\n\"\n",
        "                    \"   Install it with: !pip install -U bitsandbytes\\n\"\n",
        "                    \"   Then restart the runtime and run this cell again.\"\n",
        "                ) from e\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                \"‚ùå Cannot load QLoRA adapters on CPU!\\n\"\n",
        "                \"   The adapters were trained with 4-bit quantization (GPU only).\\n\"\n",
        "                \"   Please run this notebook on GPU (Colab: Runtime ‚Üí Change runtime type ‚Üí GPU).\"\n",
        "            )\n",
        "    \n",
        "    # Load base model - MUST use 4-bit quantization to match adapter structure\n",
        "    load_kwargs = {}\n",
        "    \n",
        "    if use_quantization:\n",
        "        # Use 4-bit quantization (same as training) - REQUIRED for adapter compatibility\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "        load_kwargs[\"quantization_config\"] = quantization_config\n",
        "        load_kwargs[\"dtype\"] = torch.bfloat16\n",
        "        load_kwargs[\"device_map\"] = \"auto\"\n",
        "        print(\"Loading model with 4-bit quantization (required for adapter compatibility)...\")\n",
        "    else:\n",
        "        # This should not happen if checks above work, but just in case\n",
        "        raise RuntimeError(\"Quantization is required but not available!\")\n",
        "    \n",
        "    # Load base model with quantization (required for adapter compatibility)\n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        **load_kwargs\n",
        "    )\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    print(\"‚úÖ Base model loaded\")\n",
        "    print(f\"Loading adapters from {adapter_repo}...\")\n",
        "    \n",
        "    # Load finetuned model (base + adapters)\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    \n",
        "    # CRITICAL: For 4-bit quantized models, we may need to merge adapters for inference\n",
        "    # Try merging adapters to ensure they're active (this is safe for inference)\n",
        "    try:\n",
        "        # Check if we can merge (some PEFT versions support this)\n",
        "        if hasattr(finetuned_model, 'merge_and_unload'):\n",
        "            print(\"‚ö†Ô∏è  Attempting to merge adapters for better inference performance...\")\n",
        "            # Note: merge_and_unload() may not work with 4-bit, so we'll try-catch it\n",
        "            try:\n",
        "                finetuned_model = finetuned_model.merge_and_unload()\n",
        "                print(\"‚úÖ Adapters merged successfully\")\n",
        "            except Exception as merge_error:\n",
        "                print(f\"‚ö†Ô∏è  Could not merge adapters (expected with 4-bit): {merge_error}\")\n",
        "                print(\"   Continuing with unmerged adapters (should still work)\")\n",
        "        else:\n",
        "            # Ensure adapters are active\n",
        "            if hasattr(finetuned_model, 'set_adapter'):\n",
        "                finetuned_model.set_adapter('default')\n",
        "                print(\"‚úÖ Adapters set to 'default'\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Note: {e}\")\n",
        "        print(\"   Continuing with adapters as loaded...\")\n",
        "    \n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"‚úÖ Finetuned model loaded\")\n",
        "    \n",
        "    # DIAGNOSTIC: Verify adapters are actually loaded and active\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ADAPTER DIAGNOSTICS\")\n",
        "    print(\"=\"*60)\n",
        "    try:\n",
        "        # Check if adapters are present\n",
        "        if hasattr(finetuned_model, 'peft_config'):\n",
        "            print(f\"‚úÖ Adapters found: {list(finetuned_model.peft_config.keys())}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: No peft_config found - adapters may not be loaded!\")\n",
        "        \n",
        "        # Check active adapters\n",
        "        if hasattr(finetuned_model, 'active_adapters'):\n",
        "            active = finetuned_model.active_adapters\n",
        "            print(f\"‚úÖ Active adapters: {active}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: Cannot check active adapters\")\n",
        "        \n",
        "        # Check adapter parameters\n",
        "        trainable_params = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in finetuned_model.parameters())\n",
        "        print(f\"‚úÖ Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "        \n",
        "        # Compare a single forward pass to see if outputs differ\n",
        "        test_text = \"It was on a dreary night of November that\"\n",
        "        test_input = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
        "        if device == \"cuda\":\n",
        "            test_input = {k: v.to(device) for k, v in test_input.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            base_output = base_model_obj(**test_input)\n",
        "            finetuned_output = finetuned_model(**test_input)\n",
        "        \n",
        "        base_logits = base_output.logits[0, -1, :10].cpu()  # First 10 logits of last token\n",
        "        finetuned_logits = finetuned_output.logits[0, -1, :10].cpu()\n",
        "        \n",
        "        logit_diff = torch.abs(base_logits - finetuned_logits).mean().item()\n",
        "        print(f\"‚úÖ Logit difference (first 10 tokens): {logit_diff:.4f}\")\n",
        "        if logit_diff < 0.001:\n",
        "            print(\"‚ö†Ô∏è  WARNING: Logits are nearly identical! Adapters may not be active.\")\n",
        "            print(\"   Possible causes:\")\n",
        "            print(\"   1. Adapters didn't learn meaningful changes during training\")\n",
        "            print(\"   2. Training loss reduction was minimal (check training logs)\")\n",
        "            print(\"   3. 4-bit quantization compatibility issue with PEFT version\")\n",
        "            print(\"   4. Adapters need to be explicitly enabled (try set_adapter if available)\")\n",
        "            print(\"   Recommendation: Check training logs to verify loss decreased during training\")\n",
        "        else:\n",
        "            print(\"‚úÖ Logits differ - adapters appear to be active\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error during diagnostics: {e}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"\\nComputing perplexity on {n_samples} samples...\")\n",
        "    print(\"‚ö†Ô∏è  This may take 5-15 minutes. Be patient!\\n\")\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Compute base model perplexity\n",
        "    print(\"Computing BASE model perplexity...\")\n",
        "    base_total_nll = 0.0\n",
        "    base_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = base_model_obj(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            base_total_nll += nll\n",
        "            base_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Base: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    base_avg_nll = base_total_nll / base_total_tokens\n",
        "    base_perplexity = torch.exp(torch.tensor(base_avg_nll)).item()\n",
        "    \n",
        "    # Compute finetuned model perplexity\n",
        "    print(\"\\nComputing FINETUNED model perplexity...\")\n",
        "    finetuned_total_nll = 0.0\n",
        "    finetuned_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = finetuned_model(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            finetuned_total_nll += nll\n",
        "            finetuned_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Finetuned: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    finetuned_avg_nll = finetuned_total_nll / finetuned_total_tokens\n",
        "    finetuned_perplexity = torch.exp(torch.tensor(finetuned_avg_nll)).item()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERPLEXITY COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Base Model:        {base_perplexity:.2f}\")\n",
        "    print(f\"Finetuned Model:   {finetuned_perplexity:.2f}\")\n",
        "    print(f\"Improvement:       {base_perplexity - finetuned_perplexity:.2f} points\")\n",
        "    print(f\"Relative Change:   {((finetuned_perplexity - base_perplexity) / base_perplexity * 100):.1f}%\")\n",
        "    print(f\"\\nSamples evaluated: {n_samples}\")\n",
        "    print(f\"Time taken: {elapsed_time/60:.1f} minutes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return {\n",
        "        'base_perplexity': base_perplexity,\n",
        "        'finetuned_perplexity': finetuned_perplexity,\n",
        "        'improvement': base_perplexity - finetuned_perplexity\n",
        "    }\n",
        "\n",
        "# Load dataset (raw text, not tokenized)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "# Get token - use environment variable or Hugging Face login\n",
        "# Option 1: Set HF_TOKEN environment variable\n",
        "# Option 2: Use: from huggingface_hub import login; login()\n",
        "hf_token = os.getenv(\"HF_TOKEN\") or HfFolder.get_token()\n",
        "if not hf_token:\n",
        "    print(\"‚ö†Ô∏è  WARNING: No HF token found. Set HF_TOKEN environment variable or use login()\")\n",
        "    print(\"   For Colab: Use Colab secrets (HF_TOKEN) or login()\")\n",
        "    hf_token = None  # Will try without token (may fail for private datasets)\n",
        "\n",
        "ds_val = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\", token=hf_token)['validation']\n",
        "\n",
        "# Evaluate\n",
        "results = eval_perplexity_with_adapters(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    ds_val,\n",
        "    n_samples=25  # Start with 25, can increase later\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Side-by-Side Generation\n",
        "\n",
        "Generate text with both models using the same prompts. Compare style, coherence, and Frankenstein-like tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generate 3-5 short continuations with both models for side-by-side comparison.\n",
        "# Hints:\n",
        "#   - Load base model and finetuned (base + adapters)\n",
        "#   - Use same prompts for both\n",
        "#   - Print outputs side-by-side or in a table\n",
        "#   - Use reasonable generation parameters (temperature, top_p)\n",
        "# Acceptance:\n",
        "#   - prints paired outputs with fixed prompts\n",
        "\n",
        "def compare_samples(base_model: str, adapter_repo: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate samples with base and finetuned models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "    from peft import PeftModel\n",
        "    import torch\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    print(\"Loading models for generation comparison...\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load base model (same as in perplexity evaluation)\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "    \n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quantization_config,\n",
        "        dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    # Load finetuned model\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"‚úÖ Models loaded for generation\\n\")\n",
        "    \n",
        "    # Generation parameters\n",
        "    generation_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "        \"do_sample\": True,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"SIDE-BY-SIDE GENERATION COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PROMPT {i}: {prompt}\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Tokenize prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if device == \"cuda\":\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate with base model\n",
        "        print(\"\\nüìò BASE MODEL:\")\n",
        "        print(\"-\" * 80)\n",
        "        with torch.no_grad():\n",
        "            base_outputs = base_model_obj.generate(**inputs, **generation_kwargs)\n",
        "        base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "        # Only show the generated part (after the prompt)\n",
        "        base_generated = base_text[len(prompt):].strip()\n",
        "        print(base_generated)\n",
        "        \n",
        "        # Generate with finetuned model\n",
        "        print(\"\\nüìó FINETUNED MODEL:\")\n",
        "        print(\"-\" * 80)\n",
        "        with torch.no_grad():\n",
        "            finetuned_outputs = finetuned_model.generate(**inputs, **generation_kwargs)\n",
        "        finetuned_text = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "        finetuned_generated = finetuned_text[len(prompt):].strip()\n",
        "        print(finetuned_generated)\n",
        "        \n",
        "        # Highlight differences (simple comparison)\n",
        "        if base_generated != finetuned_generated:\n",
        "            print(\"\\n‚úÖ Outputs differ - adapters are affecting generation\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  Outputs are identical - adapters may not be active\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# Compare\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\",\n",
        "    \"I beheld the wretch‚Äîthe miserable monster\",\n",
        "    \"Life and death appeared to me ideal bounds\"\n",
        "]\n",
        "compare_samples(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    prompts,\n",
        "    max_new_tokens=100\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "julia 1.11",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "julia",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

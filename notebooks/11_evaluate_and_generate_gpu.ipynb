{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation: Base vs Finetuned (GPU)\n",
        "\n",
        "## Comparison Goals\n",
        "\n",
        "After training, we want to:\n",
        "1. **Quantitative:** Compare perplexity (base vs finetuned)\n",
        "2. **Qualitative:** Generate samples side-by-side\n",
        "3. **Document:** Record hyperparameters, costs, latency\n",
        "\n",
        "This notebook runs on GPU for speed, but you can adapt it for CPU if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "‚úÖ bitsandbytes 0.48.2 installed successfully\n",
            "‚úÖ Version is compatible (>= 0.39.0)\n"
          ]
        }
      ],
      "source": [
        "# === INSTALL DEPENDENCIES (Run this first!) ===\n",
        "# Install/upgrade bitsandbytes - REQUIRED for loading QLoRA adapters\n",
        "# The adapters were trained with 4-bit quantization and need bitsandbytes >= 0.39.0\n",
        "\n",
        "%pip install -U bitsandbytes\n",
        "\n",
        "# Verify installation\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    from packaging import version\n",
        "    import importlib.metadata\n",
        "    bnb_version = importlib.metadata.version(\"bitsandbytes\")\n",
        "    print(f\"‚úÖ bitsandbytes {bnb_version} installed successfully\")\n",
        "    if version.parse(bnb_version) < version.parse(\"0.39.0\"):\n",
        "        print(\"‚ö†Ô∏è  WARNING: Version is < 0.39.0. You may need to restart the runtime.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Version is compatible (>= 0.39.0)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error checking bitsandbytes: {e}\")\n",
        "    print(\"   Please restart the runtime after installation and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50ab3ed56440497ebec60b45e00d075f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/383 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0424dcb66f2402294d3d8528e208863",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/278k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c6437ae1991419bacb9913f38a05ac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/20.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7df1edbad0054bb1884efa886f255c82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/456 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3c6bb45ebe441e8957f198ea0f6dcd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU detected: NVIDIA L4\n",
            "Loading base model mistralai/Mistral-7B-Instruct-v0.2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "847af2363d8d440a8c6b48dba8bbc353",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "024adb99558b4de299365db7f2d454e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e44e4ae50d446f1b2356e4e53703c53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c0c40d6663e4c7c876e317ccbf0d26b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ bitsandbytes 0.48.2 is installed and compatible\n",
            "Loading model with 4-bit quantization (required for adapter compatibility)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c54e7ffa7af448c9fe9cdabc921e238",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ecfeb13a3e34349ba4a7b0a85d68298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e4b2157408042c495765ec9ecb9c5e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4db64bda7c540b4b33526b14470b09e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "200b3bc3449e4d8a864ae91f6427a302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e673261bb684c09bc84637f2baa1a3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f49c3adc734043ce9b07ecf1b94b4393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e1dd76464cc45f0945f19c245fc158d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Base model loaded\n",
            "Loading adapters from Tuminha/mistral-frankenstein-qlora...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4968268981c14f598d8f9a39f8d2de90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/896 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cde19777a86444988df1f2973526fc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Finetuned model loaded\n",
            "\n",
            "Computing perplexity on 25 samples...\n",
            "‚ö†Ô∏è  This may take 5-15 minutes. Be patient!\n",
            "\n",
            "Computing BASE model perplexity...\n",
            "  Base: Processed 5/25 samples...\n",
            "  Base: Processed 10/25 samples...\n",
            "  Base: Processed 15/25 samples...\n",
            "  Base: Processed 20/25 samples...\n",
            "  Base: Processed 25/25 samples...\n",
            "\n",
            "Computing FINETUNED model perplexity...\n",
            "  Finetuned: Processed 5/25 samples...\n",
            "  Finetuned: Processed 10/25 samples...\n",
            "  Finetuned: Processed 15/25 samples...\n",
            "  Finetuned: Processed 20/25 samples...\n",
            "  Finetuned: Processed 25/25 samples...\n",
            "\n",
            "============================================================\n",
            "PERPLEXITY COMPARISON\n",
            "============================================================\n",
            "Base Model:        7.95\n",
            "Finetuned Model:   7.95\n",
            "Improvement:       0.00 points\n",
            "Relative Change:   0.0%\n",
            "\n",
            "Samples evaluated: 25\n",
            "Time taken: 0.6 minutes\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load base model + attach LoRA adapters; run perplexity on validation slice.\n",
        "# Hints:\n",
        "#   - Load base model in 4-bit\n",
        "#   - Use PeftModel.from_pretrained() to attach adapters\n",
        "#   - Compute perplexity on validation set (similar to notebook 04, but on GPU)\n",
        "# Acceptance:\n",
        "#   - prints ppl_base vs ppl_finetuned\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "def eval_perplexity_with_adapters(base_model: str, adapter_repo: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Evaluate perplexity with base and finetuned models.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        dataset: Validation dataset (raw text, not tokenized)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow.\")\n",
        "        print(\"   For faster evaluation, enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "        print(\"   Continuing on CPU (this may take 30-60 minutes)...\\n\")\n",
        "    else:\n",
        "        print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Loading base model {base_model}...\")\n",
        "    \n",
        "    # Load tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # CRITICAL: Adapters were trained with 4-bit quantization, so we MUST use the same structure\n",
        "    # Check if bitsandbytes is available and up-to-date (REQUIRED for loading adapters trained with QLoRA)\n",
        "    try:\n",
        "        import bitsandbytes\n",
        "        from packaging import version\n",
        "        import importlib.metadata\n",
        "        \n",
        "        # Check version - need >= 0.39.0 for 4-bit quantization\n",
        "        bnb_version = importlib.metadata.version(\"bitsandbytes\")\n",
        "        min_version = version.parse(\"0.39.0\")\n",
        "        current_version = version.parse(bnb_version)\n",
        "        \n",
        "        if current_version < min_version:\n",
        "            raise ImportError(\n",
        "                f\"‚ùå bitsandbytes version {bnb_version} is too old! Need >= 0.39.0\\n\"\n",
        "                \"   Upgrade it with: !pip install -U bitsandbytes\\n\"\n",
        "                \"   Then restart the runtime and run this cell again.\"\n",
        "            )\n",
        "        \n",
        "        use_quantization = device == \"cuda\"  # Only use quantization on GPU\n",
        "        print(f\"‚úÖ bitsandbytes {bnb_version} is installed and compatible\")\n",
        "        \n",
        "    except ImportError as e:\n",
        "        use_quantization = False\n",
        "        if device == \"cuda\":\n",
        "            error_msg = str(e)\n",
        "            if \"version\" in error_msg.lower() or \"0.39\" in error_msg:\n",
        "                # Version issue - already handled above\n",
        "                raise\n",
        "            else:\n",
        "                raise ImportError(\n",
        "                    \"‚ùå bitsandbytes is REQUIRED to load QLoRA adapters!\\n\"\n",
        "                    \"   The adapters were trained with 4-bit quantization and require the same model structure.\\n\"\n",
        "                    \"   Install it with: !pip install -U bitsandbytes\\n\"\n",
        "                    \"   Then restart the runtime and run this cell again.\"\n",
        "                ) from e\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                \"‚ùå Cannot load QLoRA adapters on CPU!\\n\"\n",
        "                \"   The adapters were trained with 4-bit quantization (GPU only).\\n\"\n",
        "                \"   Please run this notebook on GPU (Colab: Runtime ‚Üí Change runtime type ‚Üí GPU).\"\n",
        "            )\n",
        "    \n",
        "    # Load base model - MUST use 4-bit quantization to match adapter structure\n",
        "    load_kwargs = {}\n",
        "    \n",
        "    if use_quantization:\n",
        "        # Use 4-bit quantization (same as training) - REQUIRED for adapter compatibility\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "        load_kwargs[\"quantization_config\"] = quantization_config\n",
        "        load_kwargs[\"dtype\"] = torch.bfloat16\n",
        "        load_kwargs[\"device_map\"] = \"auto\"\n",
        "        print(\"Loading model with 4-bit quantization (required for adapter compatibility)...\")\n",
        "    else:\n",
        "        # This should not happen if checks above work, but just in case\n",
        "        raise RuntimeError(\"Quantization is required but not available!\")\n",
        "    \n",
        "    # Load base model with quantization (required for adapter compatibility)\n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        **load_kwargs\n",
        "    )\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    print(\"‚úÖ Base model loaded\")\n",
        "    print(f\"Loading adapters from {adapter_repo}...\")\n",
        "    \n",
        "    # Load finetuned model (base + adapters)\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"‚úÖ Finetuned model loaded\")\n",
        "    \n",
        "    # DIAGNOSTIC: Verify adapters are actually loaded and active\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ADAPTER DIAGNOSTICS\")\n",
        "    print(\"=\"*60)\n",
        "    try:\n",
        "        # Check if adapters are present\n",
        "        if hasattr(finetuned_model, 'peft_config'):\n",
        "            print(f\"‚úÖ Adapters found: {list(finetuned_model.peft_config.keys())}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: No peft_config found - adapters may not be loaded!\")\n",
        "        \n",
        "        # Check active adapters\n",
        "        if hasattr(finetuned_model, 'active_adapters'):\n",
        "            active = finetuned_model.active_adapters\n",
        "            print(f\"‚úÖ Active adapters: {active}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: Cannot check active adapters\")\n",
        "        \n",
        "        # Check adapter parameters\n",
        "        trainable_params = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in finetuned_model.parameters())\n",
        "        print(f\"‚úÖ Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "        \n",
        "        # Compare a single forward pass to see if outputs differ\n",
        "        test_text = \"It was on a dreary night of November that\"\n",
        "        test_input = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
        "        if device == \"cuda\":\n",
        "            test_input = {k: v.to(device) for k, v in test_input.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            base_output = base_model_obj(**test_input)\n",
        "            finetuned_output = finetuned_model(**test_input)\n",
        "        \n",
        "        base_logits = base_output.logits[0, -1, :10].cpu()  # First 10 logits of last token\n",
        "        finetuned_logits = finetuned_output.logits[0, -1, :10].cpu()\n",
        "        \n",
        "        logit_diff = torch.abs(base_logits - finetuned_logits).mean().item()\n",
        "        print(f\"‚úÖ Logit difference (first 10 tokens): {logit_diff:.4f}\")\n",
        "        if logit_diff < 0.001:\n",
        "            print(\"‚ö†Ô∏è  WARNING: Logits are nearly identical! Adapters may not be active.\")\n",
        "        else:\n",
        "            print(\"‚úÖ Logits differ - adapters appear to be active\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error during diagnostics: {e}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"\\nComputing perplexity on {n_samples} samples...\")\n",
        "    print(\"‚ö†Ô∏è  This may take 5-15 minutes. Be patient!\\n\")\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Compute base model perplexity\n",
        "    print(\"Computing BASE model perplexity...\")\n",
        "    base_total_nll = 0.0\n",
        "    base_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = base_model_obj(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            base_total_nll += nll\n",
        "            base_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Base: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    base_avg_nll = base_total_nll / base_total_tokens\n",
        "    base_perplexity = torch.exp(torch.tensor(base_avg_nll)).item()\n",
        "    \n",
        "    # Compute finetuned model perplexity\n",
        "    print(\"\\nComputing FINETUNED model perplexity...\")\n",
        "    finetuned_total_nll = 0.0\n",
        "    finetuned_total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            text = sample['text']\n",
        "            # Tokenize\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            if device == \"cuda\":\n",
        "                input_ids = input_ids.to(device)\n",
        "            \n",
        "            # Forward pass to get loss\n",
        "            outputs = finetuned_model(input_ids, labels=input_ids)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            finetuned_total_nll += nll\n",
        "            finetuned_total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Finetuned: Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    finetuned_avg_nll = finetuned_total_nll / finetuned_total_tokens\n",
        "    finetuned_perplexity = torch.exp(torch.tensor(finetuned_avg_nll)).item()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERPLEXITY COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Base Model:        {base_perplexity:.2f}\")\n",
        "    print(f\"Finetuned Model:   {finetuned_perplexity:.2f}\")\n",
        "    print(f\"Improvement:       {base_perplexity - finetuned_perplexity:.2f} points\")\n",
        "    print(f\"Relative Change:   {((finetuned_perplexity - base_perplexity) / base_perplexity * 100):.1f}%\")\n",
        "    print(f\"\\nSamples evaluated: {n_samples}\")\n",
        "    print(f\"Time taken: {elapsed_time/60:.1f} minutes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return {\n",
        "        'base_perplexity': base_perplexity,\n",
        "        'finetuned_perplexity': finetuned_perplexity,\n",
        "        'improvement': base_perplexity - finetuned_perplexity\n",
        "    }\n",
        "\n",
        "# Load dataset (raw text, not tokenized)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "# Get token - use environment variable or Hugging Face login\n",
        "# Option 1: Set HF_TOKEN environment variable\n",
        "# Option 2: Use: from huggingface_hub import login; login()\n",
        "hf_token = os.getenv(\"HF_TOKEN\") or HfFolder.get_token()\n",
        "if not hf_token:\n",
        "    print(\"‚ö†Ô∏è  WARNING: No HF token found. Set HF_TOKEN environment variable or use login()\")\n",
        "    print(\"   For Colab: Use Colab secrets (HF_TOKEN) or login()\")\n",
        "    hf_token = None  # Will try without token (may fail for private datasets)\n",
        "\n",
        "ds_val = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\", token=hf_token)['validation']\n",
        "\n",
        "# Evaluate\n",
        "results = eval_perplexity_with_adapters(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    ds_val,\n",
        "    n_samples=25  # Start with 25, can increase later\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Side-by-Side Generation\n",
        "\n",
        "Generate text with both models using the same prompts. Compare style, coherence, and Frankenstein-like tone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generate 3-5 short continuations with both models for side-by-side comparison.\n",
        "# Hints:\n",
        "#   - Load base model and finetuned (base + adapters)\n",
        "#   - Use same prompts for both\n",
        "#   - Print outputs side-by-side or in a table\n",
        "#   - Use reasonable generation parameters (temperature, top_p)\n",
        "# Acceptance:\n",
        "#   - prints paired outputs with fixed prompts\n",
        "\n",
        "def compare_samples(base_model: str, adapter_repo: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate samples with base and finetuned models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Base model name\n",
        "        adapter_repo: Hub repo with LoRA adapters\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "    from peft import PeftModel\n",
        "    import torch\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    print(\"Loading models for generation comparison...\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load base model (same as in perplexity evaluation)\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "    \n",
        "    base_model_obj = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quantization_config,\n",
        "        dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    base_model_obj.eval()\n",
        "    \n",
        "    # Load finetuned model\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model_obj, adapter_repo)\n",
        "    finetuned_model.eval()\n",
        "    \n",
        "    print(\"‚úÖ Models loaded for generation\\n\")\n",
        "    \n",
        "    # Generation parameters\n",
        "    generation_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "        \"do_sample\": True,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"SIDE-BY-SIDE GENERATION COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PROMPT {i}: {prompt}\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Tokenize prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if device == \"cuda\":\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate with base model\n",
        "        print(\"\\nüìò BASE MODEL:\")\n",
        "        print(\"-\" * 80)\n",
        "        with torch.no_grad():\n",
        "            base_outputs = base_model_obj.generate(**inputs, **generation_kwargs)\n",
        "        base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "        # Only show the generated part (after the prompt)\n",
        "        base_generated = base_text[len(prompt):].strip()\n",
        "        print(base_generated)\n",
        "        \n",
        "        # Generate with finetuned model\n",
        "        print(\"\\nüìó FINETUNED MODEL:\")\n",
        "        print(\"-\" * 80)\n",
        "        with torch.no_grad():\n",
        "            finetuned_outputs = finetuned_model.generate(**inputs, **generation_kwargs)\n",
        "        finetuned_text = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "        finetuned_generated = finetuned_text[len(prompt):].strip()\n",
        "        print(finetuned_generated)\n",
        "        \n",
        "        # Highlight differences (simple comparison)\n",
        "        if base_generated != finetuned_generated:\n",
        "            print(\"\\n‚úÖ Outputs differ - adapters are affecting generation\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  Outputs are identical - adapters may not be active\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# Compare\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\",\n",
        "    \"I beheld the wretch‚Äîthe miserable monster\",\n",
        "    \"Life and death appeared to me ideal bounds\"\n",
        "]\n",
        "compare_samples(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Tuminha/mistral-frankenstein-qlora\",\n",
        "    prompts,\n",
        "    max_new_tokens=100\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Harness (CPU)\n",
        "\n",
        "## Why Evaluate on CPU?\n",
        "\n",
        "Before training, we want to:\n",
        "1. **Baseline perplexity:** Measure base model performance\n",
        "2. **Sample generation:** See what the base model produces\n",
        "3. **Compare post-training:** Same metrics after finetuning\n",
        "\n",
        "Running evaluation on CPU is slow but:\n",
        "- No GPU needed for initial checks\n",
        "- Helps validate the pipeline\n",
        "- Can run locally before GPU training\n",
        "\n",
        "## Perplexity Approximation\n",
        "\n",
        "Perplexity measures how \"surprised\" the model is by the data:\n",
        "- Lower = better (model predicts data well)\n",
        "- Formula: exp(mean(negative_log_likelihood))\n",
        "\n",
        "We'll compute it on a small validation slice for speed.\n",
        "\n",
        "### What You'll See When Running This:\n",
        "\n",
        "1. **Dataset Loading:** Downloads/loads your dataset from Hugging Face Hub\n",
        "   - Train split: 456 samples\n",
        "   - Validation split: 25 samples\n",
        "\n",
        "2. **Model Download (First Time Only):** \n",
        "   - Downloads Mistral-7B (~15GB total)\n",
        "   - 3 model files (model-00001/02/03-of-00003.safetensors)\n",
        "   - This only happens once - files are cached locally\n",
        "   - **This is what you're seeing now!** ‚¨áÔ∏è\n",
        "\n",
        "3. **Model Loading:**\n",
        "   - Loads the 7B parameter model into RAM\n",
        "   - Uses CPU (no GPU needed, but slower)\n",
        "   - Takes a few minutes\n",
        "\n",
        "4. **Perplexity Calculation:**\n",
        "   - Processes validation samples one by one\n",
        "   - Computes how well model predicts each token\n",
        "   - Averages across all samples\n",
        "   - **This will take 10-30 minutes on CPU** (be patient!)\n",
        "\n",
        "### Why So Slow on CPU?\n",
        "- 7B parameters = billions of calculations per sample\n",
        "- CPU has fewer cores than GPU\n",
        "- This is why we only do this once for baseline, then use GPU for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU detected:  Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU cuda is available\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"‚úÖ GPU detected: \", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected! This will be very slow or crash.\")\n",
        "    print(\"   Please enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication for Private Dataset\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT:** The dataset `Tuminha/frankenstein-fanfic-snippets` is **private**, so you must authenticate first!\n",
        "\n",
        "### Option 1: Using .env file (Recommended for local development)\n",
        "\n",
        "Create a `.env` file in your project root with:\n",
        "```\n",
        "HF_TOKEN=your_token_here\n",
        "```\n",
        "\n",
        "Or use any of these variable names:\n",
        "- `HF_TOKEN`\n",
        "- `HUGGINGFACE_HUB_TOKEN`\n",
        "- `HUGGINGFACE_API_KEY`\n",
        "\n",
        "**Note:** Make sure `.env` is in `.gitignore` (it already is)!\n",
        "\n",
        "### Option 2: Interactive Login\n",
        "\n",
        "Run the authentication cell below to login interactively:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import login\n",
        "login()  # Paste your token when prompted\n",
        "```\n",
        "\n",
        "### Option 3: Environment Variable (Colab/Cloud)\n",
        "\n",
        "Set environment variable before running:\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
        "```\n",
        "\n",
        "### Get Your Token\n",
        "1. Go to: https://huggingface.co/settings/tokens\n",
        "2. Click \"New token\"\n",
        "3. Name it (e.g., \"colab-access\")\n",
        "4. Select \"Read\" access\n",
        "5. Copy the token\n",
        "\n",
        "**Note:** The dataset uses Parquet format (not CSV) - this is fine! `load_dataset()` handles it automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Using authentication token from: environment variable (.env or system) (length: 37)\n",
            "‚ö†Ô∏è  Could not load from Hub: NameError\n",
            "\n",
            "   Falling back to local CSV...\n",
            "   ‚úÖ Found CSV at: ../data/processed/frankenstein_cleaned.csv\n",
            "   Loaded CSV: 481 rows from ../data/processed/frankenstein_cleaned.csv\n",
            "‚úÖ Created dataset from local CSV: 456 train, 25 val\n"
          ]
        }
      ],
      "source": [
        "# Helper function to load dataset (Hub or local fallback)\n",
        "def load_dataset_with_fallback(hub_id=\"Tuminha/frankenstein-fanfic-snippets\", token=None):\n",
        "    \"\"\"\n",
        "    Try to load dataset from Hub, with authentication if needed.\n",
        "    Falls back to local CSV if Hub access fails.\n",
        "    \n",
        "    Args:\n",
        "        hub_id: Hugging Face dataset ID\n",
        "        token: Optional Hugging Face token (or use login() first)\n",
        "        \n",
        "    Returns:\n",
        "        DatasetDict with 'train' and 'validation' splits\n",
        "    \"\"\"\n",
        "    from datasets import Dataset, DatasetDict\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    \n",
        "    # Try Hub first (with authentication if needed)\n",
        "    try:\n",
        "        # Get token from environment or cached login\n",
        "        from huggingface_hub import HfFolder\n",
        "        import os\n",
        "        \n",
        "        # Try to get token from various sources (in order of priority)\n",
        "        hf_token = token  # 1. Use provided token first\n",
        "        \n",
        "        # 2. Try to load from .env file\n",
        "        if not hf_token:\n",
        "            try:\n",
        "                from dotenv import load_dotenv\n",
        "                load_dotenv()  # Load .env file if it exists\n",
        "                hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\") or os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "            except ImportError:\n",
        "                # python-dotenv not installed, skip .env loading\n",
        "                pass\n",
        "            except Exception:\n",
        "                # .env file doesn't exist or other error, continue\n",
        "                pass\n",
        "        \n",
        "        # 3. Try environment variables (already loaded from system/env)\n",
        "        if not hf_token:\n",
        "            hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\") or os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "        \n",
        "        # 4. Try to get from HfFolder (cached login via login())\n",
        "        if not hf_token:\n",
        "            try:\n",
        "                hf_token = HfFolder.get_token()\n",
        "            except Exception:\n",
        "                pass\n",
        "        \n",
        "        # Load dataset with explicit token\n",
        "        if hf_token:\n",
        "            token_source = \"provided parameter\"\n",
        "            if not token:  # If not provided as parameter, figure out source\n",
        "                if os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\") or os.getenv(\"HUGGINGFACE_API_KEY\"):\n",
        "                    token_source = \"environment variable (.env or system)\"\n",
        "                else:\n",
        "                    token_source = \"cached login (HfFolder)\"\n",
        "            print(f\"‚úÖ Using authentication token from: {token_source} (length: {len(hf_token)})\")\n",
        "            dset = load_dataset(hub_id, token=hf_token)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No token found. Trying without explicit token (may use cached)...\")\n",
        "            dset = load_dataset(hub_id)  # Will try to use cached token\n",
        "        \n",
        "        print(\"‚úÖ Loaded dataset from Hugging Face Hub\")\n",
        "        print(f\"   Train: {len(dset['train'])}, Validation: {len(dset['validation'])}\")\n",
        "        return dset\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e).lower()\n",
        "        error_type = type(e).__name__\n",
        "        \n",
        "        print(f\"‚ö†Ô∏è  Could not load from Hub: {error_type}\")\n",
        "        \n",
        "        # Check if it's an authentication/access issue\n",
        "        if \"not found\" in error_msg or \"cannot be accessed\" in error_msg or \"401\" in error_msg:\n",
        "            print(\"\\nüîê AUTHENTICATION REQUIRED\")\n",
        "            print(\"   The dataset is private. You need to authenticate first.\")\n",
        "            print(\"\\n   Run this in a cell above:\")\n",
        "            print(\"   ```python\")\n",
        "            print(\"   from huggingface_hub import login\")\n",
        "            print(\"   login()  # Enter your token when prompted\")\n",
        "            print(\"   ```\")\n",
        "            print(\"\\n   Get your token from: https://huggingface.co/settings/tokens\")\n",
        "            print(\"   (Create a token with 'read' access)\")\n",
        "            print(\"\\n   After authenticating, run this cell again.\")\n",
        "            print(\"\\n   Alternatively, you can pass a token directly:\")\n",
        "            print(\"   ```python\")\n",
        "            print(\"   dset = load_dataset_with_fallback(token='your_token_here')\")\n",
        "            print(\"   ```\")\n",
        "        \n",
        "        print(\"\\n   Falling back to local CSV...\")\n",
        "        \n",
        "        # Fallback: Try multiple possible CSV paths\n",
        "        possible_paths = [\n",
        "            \"../data/processed/frankenstein_cleaned.csv\",  # Local relative\n",
        "            \"data/processed/frankenstein_cleaned.csv\",     # Colab root\n",
        "            \"/content/data/processed/frankenstein_cleaned.csv\",  # Colab content\n",
        "            \"./data/processed/frankenstein_cleaned.csv\",   # Current dir\n",
        "        ]\n",
        "        \n",
        "        df = None\n",
        "        used_path = None\n",
        "        \n",
        "        for csv_path in possible_paths:\n",
        "            try:\n",
        "                if os.path.exists(csv_path):\n",
        "                    df = pd.read_csv(csv_path)\n",
        "                    used_path = csv_path\n",
        "                    print(f\"   ‚úÖ Found CSV at: {csv_path}\")\n",
        "                    break\n",
        "            except Exception:\n",
        "                continue\n",
        "        \n",
        "        if df is None:\n",
        "            print(f\"\\n‚ùå CSV file not found in any of these locations:\")\n",
        "            for p in possible_paths:\n",
        "                print(f\"   - {p}\")\n",
        "            print(\"\\n   üí° SOLUTIONS:\")\n",
        "            print(\"   1. Authenticate to Hub (recommended):\")\n",
        "            print(\"      from huggingface_hub import login\")\n",
        "            print(\"      login()\")\n",
        "            print(\"\\n   2. Upload CSV to Colab:\")\n",
        "            print(\"      from google.colab import files\")\n",
        "            print(\"      files.upload()  # Upload frankenstein_cleaned.csv\")\n",
        "            print(\"\\n   3. Download dataset files manually from Hub and load them\")\n",
        "            \n",
        "            raise FileNotFoundError(\n",
        "                \"Could not access dataset. Please authenticate to Hub or provide CSV file.\\n\"\n",
        "                \"The dataset exists but is private - authentication is required.\"\n",
        "            )\n",
        "        \n",
        "        print(f\"   Loaded CSV: {len(df)} rows from {used_path}\")\n",
        "        \n",
        "        # Create train/val split (same as notebook 02: 5% validation, seed=42)\n",
        "        train_df, val_df = train_test_split(df, test_size=0.05, random_state=42)\n",
        "        \n",
        "        dset = DatasetDict({\n",
        "            'train': Dataset.from_pandas(train_df),\n",
        "            'validation': Dataset.from_pandas(val_df)\n",
        "        })\n",
        "        print(f\"‚úÖ Created dataset from local CSV: {len(dset['train'])} train, {len(dset['validation'])} val\")\n",
        "        return dset\n",
        "\n",
        "# Load dataset\n",
        "# NOTE: If you get authentication error, run this first:\n",
        "# from huggingface_hub import login\n",
        "# login()  # Enter your token\n",
        "# Then run this cell again\n",
        "dset = load_dataset_with_fallback()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model mistralai/Mistral-7B-Instruct-v0.2 on CPU...\n",
            "‚ö†Ô∏è  Using DistilGPT-2 as proxy (Mistral-7B too large for CPU)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8d21399bc5140aaa2dae7c765f06456",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Compute perplexity for a small validation slice using a CPU-friendly model.\n",
        "# NOTE: Mistral-7B is too large for CPU! Use DistilGPT-2 instead.\n",
        "# Hints:\n",
        "#   - Load model and tokenizer (use \"distilgpt2\" for CPU)\n",
        "#   - Tokenize the dataset first (it's not tokenized yet!)\n",
        "#   - For each sample, compute negative log-likelihood of tokens\n",
        "#   - Sum NLL, divide by total tokens, then exp() for perplexity\n",
        "# Acceptance:\n",
        "#   - prints ppl_base on ~25 validation samples (all of them)\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "HF_TOKEN = os.getenv(\"HUGGINGFACE_API_KEY\") or os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "def cpu_perplexity_estimate(base_model: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Estimate perplexity on CPU using a smaller model (DistilGPT-2).\n",
        "    \n",
        "    NOTE: Mistral-7B requires ~28GB RAM and will crash on most CPUs.\n",
        "    We use DistilGPT-2 as a proxy baseline here.\n",
        "    Real Mistral baseline will be computed on GPU in notebook 11.\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name (use \"distilgpt2\" for CPU)\n",
        "        dataset: Validation dataset (not tokenized yet)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    print(f\"Loading model {base_model} on CPU...\")\n",
        "    print(\"‚ö†Ô∏è  Using DistilGPT-2 as proxy (Mistral-7B too large for CPU)\")\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "    model.eval()\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"Computing perplexity on {n_samples} samples...\")\n",
        "    \n",
        "    total_nll = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            # Tokenize the text\n",
        "            text = sample['text']\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids']\n",
        "            \n",
        "            # Forward pass to get logits\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            \n",
        "            # Loss is already negative log-likelihood per token (averaged)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            total_nll += nll\n",
        "            total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    # Compute perplexity\n",
        "    avg_nll = total_nll / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Baseline Perplexity (DistilGPT-2): {perplexity:.2f}\")\n",
        "    print(f\"   (computed on {n_samples} samples, {total_tokens} tokens)\")\n",
        "    print(f\"\\nüìù Note: This is DistilGPT-2 baseline, not Mistral.\")\n",
        "    print(f\"   Real Mistral baseline will be computed on GPU in notebook 11.\")\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Load dataset and compute baseline\n",
        "dset = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\")\n",
        "# Use DistilGPT-2 instead of Mistral (Mistral too large for CPU)\n",
        "cpu_perplexity_estimate(\"mistralai/Mistral-7B-Instruct-v0.2\", dset['validation'], n_samples=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mistral-7B Perplexity (GPU/Colab)\n",
        "\n",
        "**‚ö†Ô∏è Run this cell in Google Colab with GPU enabled!**\n",
        "\n",
        "This version uses Mistral-7B for the real baseline. It requires:\n",
        "- GPU runtime in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
        "- ~15GB GPU memory (T4 works fine)\n",
        "- Much faster than CPU (minutes instead of hours)\n",
        "\n",
        "Copy this cell to Colab and run it there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Mistral-7B Perplexity for GPU/Colab ===\n",
        "# Copy this cell to Google Colab and run with GPU enabled!\n",
        "# Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "def gpu_perplexity_estimate_mistral(base_model: str, dataset, n_samples: int=25):\n",
        "    \"\"\"\n",
        "    Estimate perplexity using Mistral-7B on GPU (for Colab).\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name (\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "        dataset: Validation dataset (not tokenized)\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    # Check for GPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow or crash.\")\n",
        "        print(\"   Please enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"Loading model {base_model} on {device}...\")\n",
        "    \n",
        "    # Load model on GPU with bfloat16 (saves memory)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency\n",
        "        device_map=\"auto\"  # Automatically place on GPU\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.eval()\n",
        "    \n",
        "    # Limit samples\n",
        "    n_samples = min(n_samples, len(dataset))\n",
        "    print(f\"Computing perplexity on {n_samples} samples...\")\n",
        "    \n",
        "    total_nll = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(dataset.select(range(n_samples))):\n",
        "            # Tokenize the text\n",
        "            text = sample['text']\n",
        "            encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            input_ids = encoded['input_ids'].to(device)  # Move to GPU\n",
        "            \n",
        "            # Forward pass to get logits\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            \n",
        "            # Loss is already negative log-likelihood per token (averaged)\n",
        "            nll = outputs.loss.item() * input_ids.size(1)\n",
        "            \n",
        "            total_nll += nll\n",
        "            total_tokens += input_ids.size(1)\n",
        "            \n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Processed {i + 1}/{n_samples} samples...\")\n",
        "    \n",
        "    # Compute perplexity\n",
        "    avg_nll = total_nll / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Baseline Perplexity (Mistral-7B): {perplexity:.2f}\")\n",
        "    print(f\"   (computed on {n_samples} samples, {total_tokens} tokens)\")\n",
        "    print(f\"   Device: {device}\")\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Load dataset and compute baseline\n",
        "# Uncomment and run in Colab:\n",
        "# dset = load_dataset(\"Tuminha/frankenstein-fanfic-snippets\")\n",
        "# gpu_perplexity_estimate_mistral(\"mistralai/Mistral-7B-Instruct-v0.2\", dset['validation'], n_samples=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mistral-7B Generation (GPU/Colab)\n",
        "\n",
        "**‚ö†Ô∏è Run this cell in Google Colab with GPU enabled!**\n",
        "\n",
        "Generate samples with Mistral-7B on GPU. Much faster and better quality than DistilGPT-2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Mistral-7B Generation for GPU/Colab ===\n",
        "# Copy this cell to Google Colab and run with GPU enabled!\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def gpu_sample_generate_mistral(base_model: str, prompts: list, max_new_tokens: int=100):\n",
        "    \"\"\"\n",
        "    Generate text samples using Mistral-7B on GPU (for Colab).\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name (\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    # Check for GPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow or crash.\")\n",
        "        print(\"   Please enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Loading model {base_model} on {device}...\\n\")\n",
        "    \n",
        "    # Load model on GPU with bfloat16\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.eval()\n",
        "    \n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"Prompt {i}: {prompt}\")\n",
        "        print(\"Generating (GPU - this should be fast)...\")\n",
        "        \n",
        "        # Tokenize prompt with attention_mask\n",
        "        encoded = tokenizer(\n",
        "            prompt, \n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "        \n",
        "        # Generate with attention_mask\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                encoded['input_ids'],\n",
        "                attention_mask=encoded['attention_mask'],  # Pass attention_mask\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2,  # Reduce repetition\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        continuation = generated_text[len(prompt):].strip()\n",
        "        \n",
        "        print(f\"Continuation: {continuation}\\n\")\n",
        "        print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# Test generation\n",
        "# Uncomment and run in Colab:\n",
        "# prompts = [\n",
        "#     \"It was on a dreary night of November that\",\n",
        "#     \"The monster gazed upon his creator with\"\n",
        "# ]\n",
        "# gpu_sample_generate_mistral(\"mistralai/Mistral-7B-Instruct-v0.2\", prompts, max_new_tokens=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Generation (CPU)\n",
        "\n",
        "Generating text on CPU is very slow, but useful for:\n",
        "- Seeing base model outputs\n",
        "- Validating the generation pipeline\n",
        "- Comparing before/after training\n",
        "\n",
        "We'll generate short continuations (60 tokens max) for a few prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Using distilgpt2 for CPU generation (Mistral-7B too large)\n",
            "   Real Mistral generation will be done on GPU in notebook 11.\n",
            "   Note: DistilGPT-2 wasn't trained on Frankenstein text, so outputs\n",
            "   won't match the style. This is just for testing the pipeline.\n",
            "\n",
            "Prompt 1: It was on a dreary night of November that\n",
            "Generating (CPU - this may take 30-60 seconds)...\n",
            "Continuation: it would not happen. This afternoon, my daughter and I sat down to take photos with her friends in the parking lot as she waited for them to be taken away from us by our own private plane.[7]\n",
            "The photo posted at http://www2gw0u9rZd\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt 2: The monster gazed upon his creator with\n",
            "Generating (CPU - this may take 30-60 seconds)...\n",
            "Continuation: a smile.\n",
            "It was the most interesting thing to behold in my life: he had an almost supernatural ability and this one, which is more than anything I've ever seen before; it's hard to imagine how many of them have even heard about him now! But for as long as we remember\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generation wrapper using DistilGPT-2 on CPU for 1-2 short prompts.\n",
        "# NOTE: Using DistilGPT-2 instead of Mistral (Mistral too large for CPU).\n",
        "# Hints:\n",
        "#   - Load model and tokenizer (use \"distilgpt2\")\n",
        "#   - Use model.generate() with max_new_tokens\n",
        "#   - Decode and print outputs\n",
        "#   - Warn about CPU latency\n",
        "# Acceptance:\n",
        "#   - prints 2 short continuations; warns about CPU latency\n",
        "\n",
        "def cpu_sample_generate(base_model: str, prompts: list, max_new_tokens: int=60):\n",
        "    \"\"\"\n",
        "    Generate text samples on CPU using DistilGPT-2 (Mistral too large for CPU).\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name (use \"distilgpt2\" for CPU)\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    print(f\"‚ö†Ô∏è  Using {base_model} for CPU generation (Mistral-7B too large)\")\n",
        "    print(\"   Real Mistral generation will be done on GPU in notebook 11.\")\n",
        "    print(\"   Note: DistilGPT-2 wasn't trained on Frankenstein text, so outputs\")\n",
        "    print(\"   won't match the style. This is just for testing the pipeline.\\n\")\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    \n",
        "    # Fix: Set pad_token properly (DistilGPT-2 doesn't have one by default)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"Prompt {i}: {prompt}\")\n",
        "        print(\"Generating (CPU - this may take 30-60 seconds)...\")\n",
        "        \n",
        "        # Fix: Tokenize with attention_mask (like notebook 3)\n",
        "        # This explicitly creates attention_mask to avoid the warning\n",
        "        encoded = tokenizer(\n",
        "            prompt, \n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,  # No padding needed for single prompt\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        \n",
        "        # Generate with attention_mask (fixes the warning)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                encoded['input_ids'],\n",
        "                attention_mask=encoded['attention_mask'],  # Fix: Pass attention_mask\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.8,  # Slightly higher for more variety\n",
        "                top_p=0.9,  # Nucleus sampling\n",
        "                repetition_penalty=1.2,  # Reduce repetition\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        continuation = generated_text[len(prompt):].strip()\n",
        "        \n",
        "        print(f\"Continuation: {continuation}\\n\")\n",
        "        print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# Test generation\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\"\n",
        "]\n",
        "cpu_sample_generate(\"distilgpt2\", prompts, max_new_tokens=60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

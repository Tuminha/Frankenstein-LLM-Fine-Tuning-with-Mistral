{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Harness (CPU)\n",
        "\n",
        "## Why Evaluate on CPU?\n",
        "\n",
        "Before training, we want to:\n",
        "1. **Baseline perplexity:** Measure base model performance\n",
        "2. **Sample generation:** See what the base model produces\n",
        "3. **Compare post-training:** Same metrics after finetuning\n",
        "\n",
        "Running evaluation on CPU is slow but:\n",
        "- No GPU needed for initial checks\n",
        "- Helps validate the pipeline\n",
        "- Can run locally before GPU training\n",
        "\n",
        "## Perplexity Approximation\n",
        "\n",
        "Perplexity measures how \"surprised\" the model is by the data:\n",
        "- Lower = better (model predicts data well)\n",
        "- Formula: exp(mean(negative_log_likelihood))\n",
        "\n",
        "We'll compute it on a small validation slice for speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Compute perplexity for a small validation slice using the base model (CPU).\n",
        "# Hints:\n",
        "#   - Load model and tokenizer\n",
        "#   - For each sample, compute negative log-likelihood of tokens\n",
        "#   - Sum NLL, divide by total tokens, then exp() for perplexity\n",
        "#   - Use model.generate() is NOT needed hereâ€”we're computing likelihood\n",
        "# Acceptance:\n",
        "#   - prints ppl_base on ~100-200 samples\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def cpu_perplexity_estimate(base_model: str, tokenized_ds, n_samples: int=200):\n",
        "    \"\"\"\n",
        "    Estimate perplexity on CPU (slow but works without GPU).\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name\n",
        "        tokenized_ds: Tokenized validation dataset\n",
        "        n_samples: Number of samples to evaluate\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Load dataset and compute baseline\n",
        "# dset = load_dataset(\"YOURUSER/frankenstein-fanfic-snippets\")  # or local\n",
        "# cpu_perplexity_estimate(\"mistralai/Mistral-7B-Instruct-v0.2\", dset['validation'], n_samples=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Generation (CPU)\n",
        "\n",
        "Generating text on CPU is very slow, but useful for:\n",
        "- Seeing base model outputs\n",
        "- Validating the generation pipeline\n",
        "- Comparing before/after training\n",
        "\n",
        "We'll generate short continuations (60 tokens max) for a few prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Generation wrapper using base model on CPU for 1-2 short prompts.\n",
        "# Hints:\n",
        "#   - Load model and tokenizer\n",
        "#   - Use model.generate() with max_new_tokens\n",
        "#   - Decode and print outputs\n",
        "#   - Warn about CPU latency\n",
        "# Acceptance:\n",
        "#   - prints 2 short continuations; warns about CPU latency\n",
        "\n",
        "def cpu_sample_generate(base_model: str, prompts: list, max_new_tokens: int=60):\n",
        "    \"\"\"\n",
        "    Generate text samples on CPU (slow but works without GPU).\n",
        "    \n",
        "    Args:\n",
        "        base_model: Model name\n",
        "        prompts: List of prompt strings\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Test generation\n",
        "prompts = [\n",
        "    \"It was on a dreary night of November that\",\n",
        "    \"The monster gazed upon his creator with\"\n",
        "]\n",
        "# cpu_sample_generate(\"mistralai/Mistral-7B-Instruct-v0.2\", prompts, max_new_tokens=60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

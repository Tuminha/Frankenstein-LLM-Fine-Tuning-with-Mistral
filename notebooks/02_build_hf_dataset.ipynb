{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Hugging Face Dataset\n",
        "\n",
        "## Why `datasets.Dataset`?\n",
        "\n",
        "Hugging Face's `datasets` library provides:\n",
        "- **Efficient data loading:** Lazy loading, memory mapping, streaming\n",
        "- **Built-in splits:** Easy train/validation splitting\n",
        "- **Hub integration:** Push/pull datasets seamlessly\n",
        "- **Tokenization helpers:** Works seamlessly with tokenizers\n",
        "\n",
        "For our hybrid workflow, converting to `DatasetDict` lets us:\n",
        "1. Push to Hub from CPU\n",
        "2. Pull in Colab for GPU training\n",
        "3. Maintain train/val splits automatically\n",
        "\n",
        "## Train/Validation Split Strategy\n",
        "\n",
        "We use a small validation set (5% default) because:\n",
        "- Language models need large training sets\n",
        "- Validation is mainly for monitoring overfitting\n",
        "- Small val set is sufficient for perplexity checks\n",
        "\n",
        "Use a **fixed random seed** for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 456, Val: 25\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Convert DataFrame to DatasetDict with train/validation split.\n",
        "# Hints:\n",
        "#   - Use datasets.Dataset.from_pandas() to convert DataFrame\n",
        "#   - Use .train_test_split() with test_size=val_split\n",
        "#   - Set seed for reproducibility\n",
        "# Acceptance:\n",
        "#   - DatasetDict with expected split sizes (val ~ config.val_split)\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def to_hf_dataset(df, val_split: float, seed: int=42):\n",
        "    \"\"\"\n",
        "    Convert DataFrame to Hugging Face DatasetDict with train/val split.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with 'text' column\n",
        "        val_split: Fraction for validation (e.g., 0.05)\n",
        "        seed: Random seed for splitting\n",
        "        \n",
        "    Returns:\n",
        "        DatasetDict: Dictionary with 'train' and 'validation' splits\n",
        "    \"\"\"\n",
        "    # Convert entire DataFrame to Dataset first\n",
        "    full_dataset = Dataset.from_pandas(df)\n",
        "    \n",
        "    # Use HF's built-in train_test_split method\n",
        "    split_dataset = full_dataset.train_test_split(test_size=val_split, seed=seed)\n",
        "    \n",
        "    # Rename 'test' to 'validation' for clarity\n",
        "    return DatasetDict({\n",
        "        'train': split_dataset['train'],\n",
        "        'validation': split_dataset['test']\n",
        "    })\n",
        "    \n",
        "\n",
        "# Load cleaned data and convert\n",
        "df = pd.read_csv(\"../data/processed/frankenstein_cleaned.csv\")\n",
        "dset = to_hf_dataset(df, val_split=0.05, seed=42)\n",
        "print(f\"Train: {len(dset['train'])}, Val: {len(dset['validation'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pushing to Hugging Face Hub\n",
        "\n",
        "Pushing to the Hub makes the dataset portable:\n",
        "- Pull it in Colab without file transfers\n",
        "- Share with collaborators\n",
        "- Version control your data\n",
        "\n",
        "**Note:** You'll need a Hugging Face token. Set it as an environment variable or use `huggingface_hub.login()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing token\n",
            "Logged in successfully\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27fc6e2728224db8a7e618c645042852",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a06bfb77e864b7fbb2fd71546eca8a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83e5d3becc494bd8a0071874000a5eeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308721c4d94b437b85ccc26d1ee33acb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41704d324d6344edb205ba7587c0cc19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2faa56f56f5047e8a628325d51863542",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f68898ce5d9d417db1b11b81330e3c0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83dfeb914a104c43abae5d3c07b27caa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset pushed to Tuminha/frankenstein-fanfic-snippets\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# (Optional) Push dataset to the HF Hub.\n",
        "# Hints:\n",
        "#   - Require env var or prompt for token; use push_to_hub with repo_id\n",
        "#   - Handle authentication (huggingface_hub.login() or token from env)\n",
        "#   - Use private=True if you want to keep it private\n",
        "# Acceptance:\n",
        "#   - dataset appears on the Hub; or skipped cleanly if not configured\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def maybe_push_dataset(dset, repo_id: str):\n",
        "    \"\"\"\n",
        "    Optionally push dataset to Hugging Face Hub.\n",
        "    \n",
        "    Args:\n",
        "        dset: DatasetDict to push\n",
        "        repo_id: Hub repository ID (e.g., \"username/dataset-name\")\n",
        "    \"\"\"\n",
        "    load_dotenv()\n",
        "    token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "    if not token:\n",
        "        token = input(\"Enter your Hugging Face token: \")\n",
        "        os.environ[\"HUGGINGFACE_API_KEY\"] = token\n",
        "    else:\n",
        "        print(\"Using existing token\")\n",
        "    login(token=token)\n",
        "    print(\"Logged in successfully\")\n",
        "    \n",
        "    # Actually push the dataset to the Hub\n",
        "    dset.push_to_hub(repo_id, private=True)\n",
        "    print(f\"Dataset pushed to {repo_id}\")\n",
        "    \n",
        "\n",
        "# Push if configured\n",
        "repo_id = \"Tuminha/frankenstein-fanfic-snippets\"  # Update this!\n",
        "maybe_push_dataset(dset, repo_id)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Hugging Face Dataset\n",
        "\n",
        "## Why `datasets.Dataset`?\n",
        "\n",
        "Hugging Face's `datasets` library provides:\n",
        "- **Efficient data loading:** Lazy loading, memory mapping, streaming\n",
        "- **Built-in splits:** Easy train/validation splitting\n",
        "- **Hub integration:** Push/pull datasets seamlessly\n",
        "- **Tokenization helpers:** Works seamlessly with tokenizers\n",
        "\n",
        "For our hybrid workflow, converting to `DatasetDict` lets us:\n",
        "1. Push to Hub from CPU\n",
        "2. Pull in Colab for GPU training\n",
        "3. Maintain train/val splits automatically\n",
        "\n",
        "## Train/Validation Split Strategy\n",
        "\n",
        "We use a small validation set (5% default) because:\n",
        "- Language models need large training sets\n",
        "- Validation is mainly for monitoring overfitting\n",
        "- Small val set is sufficient for perplexity checks\n",
        "\n",
        "Use a **fixed random seed** for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Convert DataFrame to DatasetDict with train/validation split.\n",
        "# Hints:\n",
        "#   - Use datasets.Dataset.from_pandas() to convert DataFrame\n",
        "#   - Use .train_test_split() with test_size=val_split\n",
        "#   - Set seed for reproducibility\n",
        "# Acceptance:\n",
        "#   - DatasetDict with expected split sizes (val ~ config.val_split)\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "def to_hf_dataset(df, val_split: float, seed: int=42):\n",
        "    \"\"\"\n",
        "    Convert DataFrame to Hugging Face DatasetDict with train/val split.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with 'text' column\n",
        "        val_split: Fraction for validation (e.g., 0.05)\n",
        "        seed: Random seed for splitting\n",
        "        \n",
        "    Returns:\n",
        "        DatasetDict: Dictionary with 'train' and 'validation' splits\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Load cleaned data and convert\n",
        "df = pd.read_csv(\"data/processed/frankenstein_cleaned.csv\")\n",
        "dset = to_hf_dataset(df, val_split=0.05, seed=42)\n",
        "print(f\"Train: {len(dset['train'])}, Val: {len(dset['validation'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pushing to Hugging Face Hub\n",
        "\n",
        "Pushing to the Hub makes the dataset portable:\n",
        "- Pull it in Colab without file transfers\n",
        "- Share with collaborators\n",
        "- Version control your data\n",
        "\n",
        "**Note:** You'll need a Hugging Face token. Set it as an environment variable or use `huggingface_hub.login()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# (Optional) Push dataset to the HF Hub.\n",
        "# Hints:\n",
        "#   - Require env var or prompt for token; use push_to_hub with repo_id\n",
        "#   - Handle authentication (huggingface_hub.login() or token from env)\n",
        "#   - Use private=True if you want to keep it private\n",
        "# Acceptance:\n",
        "#   - dataset appears on the Hub; or skipped cleanly if not configured\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "def maybe_push_dataset(dset, repo_id: str):\n",
        "    \"\"\"\n",
        "    Optionally push dataset to Hugging Face Hub.\n",
        "    \n",
        "    Args:\n",
        "        dset: DatasetDict to push\n",
        "        repo_id: Hub repository ID (e.g., \"username/dataset-name\")\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Push if configured\n",
        "repo_id = \"YOURUSER/frankenstein-fanfic-snippets\"  # Update this!\n",
        "maybe_push_dataset(dset, repo_id)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
